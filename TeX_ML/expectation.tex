\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bPi}{\mathbf{\Pi}}
\newcommand{\bQ}{{\mathbf{Q}}}
\newcommand{\bq}{{\mathbf{q}}}
\newcommand{\bK}{{\mathbf{K}}}
\newcommand{\bU}{{\mathbf{U}}}
\newcommand{\bS}{{\mathbf{S}}}
\newcommand{\bV}{{\mathbf{V}}}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{tikz}
\usepackage{cancel}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
\begin{document}
\title{Expectation}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Expectation}
Let a fair die be tosses. The set out outcomes is ${1,2,3,4,5,6}$. $E(x)$ is $\frac{1}{6}(1+2+3+4+5+6)=3.5$. What is $E(xE(x))$. One way to understand it is to consider $xE(x)$ as another random variable capable of taking values ${3.5,7,10.5,14,17.5,21}$. So, $E(xE(x))=\frac{1}{6}(3.5+7+10.5+14+17.5+21)=3.5*\frac{1}{6}(1+2+3+4+5+6)=3.5^2=E(x)^2$. Basiclly, $E(x)$ is number, not a random variable, and we can take it outside to get $E(x)E(x)$ 
\section{Total Expectation}
From \url{https://en.wikipedia.org/wiki/Law_of_total_expectation}.
\beq
E(X) = E(E(X|Y))
\eeq
Proof:
\ber
E(E(X|Y)) &=& E\Big[\sum_{x}xP(X=x|Y)\Big] \\
&=& \sum_y\Big[\sum_{x}xP(X=x|Y)\Big]P(Y=y) \\
&=& \sum_y\sum_xxP(X=x,Y=y) \\
&=& \sum_x x \sum_yP(X=x,Y=y) \text{ assuming sums can be interchanged } \\
&=& \sum_x x P(X=x) \\
&=& E(X)
\eer
Note: $P(X=y,Y=y)=P(X=x|Y=y)P(y)$ \text { because } $P(x\cup{y}) = P(x|y)p(y)$.
\section{Covariance and expectation}
\ber
\text{cov}(x,y) &=& E((x-E(x))(y-E(y))) \\
&=& E(xy - xE(y) - yE(x) + E(x)(E(y))) \\
&=& E(xy)- E(xE(y)) - E(yE(x)) + E(E(x)E(y)) \\
&=& E(xy) -2E(x)E(y) + E(x)E(y) \\
&=& E(xy) - E(x)E(y)
\eer
\section{Expectation and independence}
$x,y$ are independent. Therefore, $P(x,y)=P(x)P(y)$
\ber
E(xy) = \sum_{x}\sum_y xy P(xy) =  \sum_{x}\sum_y xy P(x)P(y) = \sum_x xP(x) \sum_y yP(y) = E(x)E(y)
\eer
\section{Expectation and linearity}
Graham Kemp's comment on \url{https://math.stackexchange.com/questions/1810365/proof-of-linearity-for-expectation-given-random-variables-are-dependent}.
\ber
Y = X_1 + X_2 \\
E(Y) &=& \sum_{x_1,x_2}(x_1 + x_2)P(x_1,x_2) \\
&=& \sum_{x_1}\sum_{x_2}x_1P(x_1,x_2) + \sum_{x_1}\sum_{x_2}x_2P(x_1,x_2) \\
&=& \sum_{x_1}x_1\sum_{x_2}P(x_1,x_2)  + \sum_{x_2}x_2\sum_{x_1}P(x_1,x_2) \\
&=& \sum_{x_1}x_1P(x_1) + \sum_{x_2}x_2P(x_2) \text{...}P(x_2)=\sum_{x_1}P(x_1,x_2)\\
&=& E(X_1) + E(X_2)
\eer
\section{Expectation of a constant}
Interpret $E(c)$ as the random variable $x$ taking the value $c$ with 100\% probability. So,
\ber
E(c) = \sum_{x}xP(x) = cP(c) = c*1 = c
\eer
In the continuous case, the pdf is a dirac delta function $\delta(x-c)$, which yields zero probability for any interval which does not contain $c$. The expectation is
\ber
E(c) = \int xP(c) = \int x \delta(x-c) = c
\eer
\section{Variance of addition}
\ber
Y &=& X_1 + X_2 \\
\text{var}(Y) &=& E(Y^2) - (E(Y))^2\\
&=& E(X_1^2 + 2X_1X_2 + X_2^2) - (E(X_1) + E(X_2))^2 \\
&=& E(X_1^2) + 2E(X_1X_2) + E(X_2^2) - (E(X_1)^2 + E(X_2)^2 + 2E(X_1)E(X_2)) \\bg
&=& E(X_1^2) - E(X_1)^2 + E(X_2^2) - E(X_2)^2 +2(E(X_1X_2)-E(X_1)E(X_2)) \\
&=& \text{var}(X_1) + \text{var}(X_2) +2\text{cov}(X_1,X_2)
\eer
\end{document}

