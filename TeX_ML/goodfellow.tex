\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bD}{{\mathbf{D}}}
\newcommand{\notimplies}{\;\not\!\!\!\implies}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{tikz}
\usepackage{cancel}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
\begin{document}
\title{Notes from Goodfellow}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{PCA Section 2.12}
Given a decoder $\pmb{D}$, what should the encoder be? The idea is that of all the points $\pmb{c}$ that one could map $\pmb{x}$ to, we should choose the point $\pmb{c}^*$ that minimizes $\|\pmb{x}-g(\pmb{c})\|_2$. Taking derivatives we find that the minimizer satisfies $\pmb{c}^* = \pmb{D}^T\pmb{x}$. So, $\pmb{D}^T$ is our encoder.
\section{Covariance and independence Section 3.8}
\ber
\text{ indepdent variables } & & \implies \text{ zero covariance }\\
\text{ non-zero covariance } & & \implies \text{ dependent variables }\\
\text{ zero covariance }     & & \implies \text{ no linear dependence, can be non-linear dependence }\\
\text{ dependent variables } & & \implies \text{ possible zero covariance, see below}\\
\eer
$\mathrm{x} \in [-1,1]$ be a uniform random variable. Hence, $P(x)=\frac{1}{2}$. $s$ is a discrete random variable taking values $1$ or $-1$ with probability $\frac{1}{2}$. We generate a new random variable $y=sx$. Clearly, $x$ and $y$ are not independent, because $x$ completely determines the magnitude of $y$. However, $\text{Cov}(x,y)=0$. We will prove this. \\
Clearly $x$ and $s$ are independent. Hence,
\beq
\mathbb{E}(y) = \mathbb{E}(xs) = \mathbb{E}(x)\mathbb{E}(s)
\eeq
\beq
\mathbb{E}(x) = \int_{-1}^{1}P(x)xdx = \int_{-1}^{1}\frac{1}{2}xdx = \frac{1}{2}\Big[\frac{x^2}{2}\Big]_{-1}^{1} = 0
\eeq
\beq
\mathbb{E}(s) = P(s=1)1 + P(s=-1)(-1) = 0.5*1+(0.5)*(-1) = 0
\eeq
Therefore $\mathbb{E}(y)=0$
\beq
\text{Cov}(x,y) = \mathbb{E}\Big[(x-\mathbb{E}(x))(y-\mathbb{E}(y))\Big] = \mathbb{E}\Big[xy\Big] = \mathbb{E}\Big[sx^2\Big]
\eeq
Again, since $x$ and $s$ are independent
\beq
\text{Cov}(x,y) = \mathbb{E}\Big[sx^2\Big] =  \mathbb{E}(x^2)\mathbb{E}(s)=0
\eeq
$x$ and $y$ are dependent but their covariance is zero.
\section{ Multinomial/Multinoulli distribution: 3.9.2 }
From \url{https://www.euanrussano.com/post/probability/multinoulli_multinomial/}.
\subsection{Expectation of product}
Let $x,y$ be independent random variables. Then
\ber
\mathbb{E}(xy) &=& \int \int P(x,y)xy dxy = \int\int P_1(x)P_2(y) xy dxdy \\
               &=& \int P_1(x)xdx \int P_2(y)ydy = \mathbb{E}(x)\mathbb{E}(y)
\eer
I think the critical part is that $P(x,y)=P_1(x)P_2(y)$ if $x,y$ are independent.
\subsection{Multinoulli distribution}
This distribution is also called categorial distribution, since it can be used to model events with K possible outcomes. Bernoulli distribution can be seen as a specific case of Multinoulli, where the number of possible outcomes K is 2. In machine learning, the multinoullli can used to model the expected class of one sample into a set of K classes. For instance, one may want to predict to which specie  in the set  a flower belongs based on its attribute. Then species K follow a multinoulli distribution.\\
Consider the $p(x=k)$ the probability that the sample $x$ belongs to class k. Here  $x$ could be the attributes of a flower in the example above, or one side of a die in the roll of it. If the set of classes is $K \,\in \, 1,2,3,\cdots,K$ then the probability of each outcome can be written as:
\ber
p(x=1) &=& p_1\\
p(x=2) &=& p_2\\
... & &\\
p(x=K) &=& p_K
\eer
Naturally, the probabilities sum to 1.0. ($\sum_{i}^{K}p(x=i)=1.0$). Coming back to the example of flowers classification, say that for a sample  the following probabilites where obtained for each of the 3 classes.
\ber
p(x=1) &=& 0.1\\
p(x=2) &=& 0.3\\
... & &\\
p(x=K) &=& 0.6
\eer
Clearly $\sum_{i}^{K}p(x=i)=1.0$ and one would say that the sample is most probable from class 3.
\subsection{Multinomial distribution}
The multinomial distribution describes repeated and independent Multinoulli trials. It is a generalization of he binomial distribution, where there may be K possible outcomes (instead of binary. As an example in machine learning and NLP (natural language processing), multinomial distribution models the counts of words in a document. Similar to Multinoulli, we say that a sample $x$ may take $K$ possible outcomes, each one with prabability $p_K$ after n successive trials. The probability (pmf) of a certain (particular) outcome can be modeled using the formula:
\beq
p(x=k) = \frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots{p_k^{x_k}}
\eeq
Where $n$ is the number of trials, $x_i$ is the number of times event $i$ occurs and is the probability $p_i$ of event $i$ at each independent trial.\\
As an example, consider a problem which can take 3 outcomes at each trial. The probability of obtaining one specific outcomes can be written as:
\beq
p(x=k) = \frac{n!}{x_1!x_2!x_3!}p_1^{x_1}p_2^{x_2}p_3^{x_3}
\eeq
This can be used to model, for instance, the probability of one specific outcome on a chess tournment. Say that we want to determine what is the probability that, after 12 games, player 1 will have 7 wins, player 2 will have 2 wins and the remaining games will finish in draw. For that, suppose that the probability that Player 1 wins is 0.4, Player 2 is 0.35 and the tie has probability 0.25. Therefore we have,
\ber
n &=& 12\\
x_1 &=& 7\\
x_2 &=& 2\\
x_3 &=& 3 \\
p1  &=& 0.4\\
p2  &=& 0.35\\
p3  &=& 0.25
\eer
Replacing that in the formula shown above:
\ber
p(x=k) = \frac{12!}{7!2!3!}{0.4}^7{0.35}^2{0.25}^3 = 0.0248
\eer
\section{Equation 3.52}
$a$ and $c$ are independent given $b$
\beq
p(a,b,c) = p(a)p(b|a)p(c|b)
\eeq
$p(a,b,c)$ seems to be the probability of $a$ and $b$ and $c$ occurring. The rhs can be simplified to
\beq
p(a,b)p(c|b) = p(a,b)p(c|a,b)  \text{ because of independence}
\eeq
which is the same thing as $p(a,b,c)$.
\section{Equation 5.1}
\beq
p(\mathbf{x}) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)
\eeq
RHS =
\beq
p(x_1\cap x_2)p(x_3|x_1x_2) = p(x_1\cap x_2 \cap x_3) = p(\mathbf{x})
\eeq
\section{Biased and unbiased estimators}
\subsection{Bernoulli distribution}
Random variable takes value $k=1$ with prob $\theta$ and $k=0$ with prob $1-\theta$. PMF is $\theta^k(1-\theta)^{1-k}$ assuming $0^0=1$. The true mean is
\beq
\theta = 1*P(1) + 0*P(0) = \theta
\eeq
The estimator for the mean is
\beq
\hat{\theta}_m = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}
\eeq
Now,
\beq
\mathbb{E}(\hat{\theta}_m) = \frac{1}{m}\sum_{i=1}^{m}\mathbb{E}(x^{(i)})
\eeq
Now
\beq
\mathbb{E}(x^{(i)}) = 1*P(1) + 0*P(0) = \theta \text{ ... } x^{(i)} \text{ can take values 0 or 1 only. }
\eeq
So,
\beq
\mathbb{E}(\hat{\theta}_m) = \frac{1}{m}\sum_{i=1}^{m}\theta = \theta \text{ ...the true mean. Therefore this estimator is unbiased.}  
\eeq
\subsection{Biased estimator for the variance of Gaussian}
From \url{https://dawenl.github.io/files/mle_biased.pdf} Dawen Liang CMU.
The estimator
\beq
\hat{\mu}_{m} = \frac{1}{m}\sum_{i=1}^{m} x^{(i)}
\eeq
is unbiased for the mean of the Gaussian. The biased estimator for the variance of the Gaussian is
\ber
\hat{\sigma}^2_m &=& \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\hat{\mu}_m)^2 = \frac{1}{m}\Big[\sum_{i=1}^{m}(x^{(i)})^2 - 2\hat{\mu}_{m}\sum_{i=1}^{m}x^{(i)} + \hat{\mu}_m(m)\Big]\\
&=& \frac{1}{m}\Big[\sum_{i=1}^{m}(x^{(i)})^2 - m\hat{\mu}_m^2\Big] \label{eqn:biasedgaussestim}
\eer
The expected value of this estimator is
\ber
\mathbb{E}(\hat{\sigma}^2_m) = \frac{1}{m}\Big[\sum_{i=1}^{m}\mathbb{E}((x^{(i)})^2)\Big]-\mathbb{E}(\hat{\mu}_m^2)
\eer
Using $\mathbb{E}((x^{(i)})^2)=\mathbb{E}(x^2) \,\,\forall\,\,i$
\beq
\label{eqn:expvargaussestim}
\mathbb{E}(\hat{\sigma}^2_m) = \mathbb{E}(x^2) - \mathbb{E}(\hat{\mu}^2_m)
\eeq
The true variances of $x$ and $\hat{\mu}^2$ are:
\ber
\sigma^2 = \mathbb{E}(x^2) - (\mathbb{E}(x))^2 \implies (\mathbb{E}(x^2) = \sigma^2 + \mu^2 \cdots \text{because } E(x)=\mu\\
\sigma^2_{\hat{\mu}} = \mathbb{E}(\hat{\mu}^2) - \mathbb{E}(\hat{\mu}_m)^2 \implies \mathbb{E}(\hat{\mu}^2_m) =  \sigma^2_{\hat{\mu}} + \mu^2 \cdots \text{because } E(\hat{\mu}_m)=\mu
\eer
Using the above equations in (\ref{eqn:expvargaussestim}),
\beq
\label{eqn:expvargaussestim2}
\mathbb{E}(\hat{\sigma}^2_m)  = \sigma^2 - \sigma^2_{\hat{\mu}}
\eeq
But,
\ber
\sigma^2_{\hat{\mu}_m} &=& \text{var}\Big( \frac{1}{m}\sum_{i=1}^{m} x^{(i)}\Big) \\
&=& \frac{1}{m^2}\text{var}\Big(\sum_{i=1}^{m} x^{(i)}\Big) \\
&=& \frac{1}{m^2}\sum_{i=1}^{m}\text{var}(x^{(i)}) = \frac{1}{m^2}(m\text{var}(x^{(i)}))\\
&=&\frac{1}{m}\text{var}(x^{(i)})= \frac{\sigma}{m}
\eer
Using the above equation in (\ref{eqn:expvargaussestim}),
\beq
\mathbb{E}(\hat{\sigma}^2_m) = \sigma^2 - \frac{\sigma^2}{m} = \Big(1-\frac{1}{m}\Big)\sigma^2
\eeq
Since $\mathbb{E}(\hat{\sigma}^2_m)\ne \sigma^2$, equation (\ref{eqn:biasedgaussestim}) is not an unbiased estimator for the variance.
\section{Equation 5.46}
\ber
\sqrt{\text{var}\Big[\frac{1}{m}\sum_{i=1}^{m}x^{(i)}\Big]} = \sqrt{\frac{1}{m^2}\sum_{i=1}^{m}\text{var}(x^{(i)})} = \sqrt{\frac{1}{m^2}m\text{var}(x)} = \sqrt{\frac{\sigma^2}{m}} = \frac{\sigma}{\sqrt{m}}
\eer
We have used $\text{var}(x^{(i)})=\sigma^2\,\forall\,i$
\section{Deciding which algorithm is better Pg. 128}
In machine learning experiments, it is common to say that algorithm A is better than algorithm B if the upper bound of the 95\% conﬁdence interval for the error of algorithm A is less than the lower bound of the 95\% conﬁdence interval for the error of algorithm B.
\section{Equation 5.54: Proof}
See \url{https://stats.stackexchange.com/questions/123320/mse-decomposition-to-variance-and-bias-squared} and \url{https://en.wikipedia.org/wiki/Mean_squared_error#Proof_of_variance_and_bias_relationship}
We want to prove
\beq
\text{MSE} = \mathbb{E}[(\hat{\theta}_m - \theta)^2] = \text{Bias}(\hat{\theta}_{m})^2 + \text{var}(\hat{\theta}_{m})
\eeq
$\theta$ is an known, true, non-random parameter. Proof:
\ber
\text{MSE} &=& \mathbb{E}[(\hat{\theta}_m-\theta)^2] \\
&=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m)+\mathbb{E}(\hat{\theta}_m)-\theta)^2]\\
&=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2 - 2(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))(\mathbb{E}(\hat{\theta}_m)-\theta) +  (\mathbb{E}(\hat{\theta}_m)-\theta)^2]\\
&=&  \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2] - 2\mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))(\mathbb{E}(\hat{\theta}_m)-\theta)] + \mathbb{E}[(\mathbb{E}(\hat{\theta}_m)-\theta)^2]
\eer
In the second and third term,
\beq
\mathbb{E}(\hat{\theta}_m)-\theta
\eeq
is constant, because $\theta$ is a constant true parameter and $\mathbb{E}(\hat{\theta}_m)$ is no-longer random, and hence can be taken out of the expectation. The equation then becomes,
\ber
\text{MSE} &=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2] - 2(\mathbb{E}(\hat{\theta}_m)-\theta)\mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))] + (\mathbb{E}(\hat{\theta}_m)-\theta)^2 \\
&=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2] - 2(\mathbb{E}(\hat{\theta}_m)-\theta)[\mathbb{E}(\hat{\theta}_m)-\mathbb{E}(\mathbb{E}(\hat{\theta}_m))] + (\mathbb{E}(\hat{\theta}_m)-\theta)^2 \\
\eer
But
\beq
\mathbb{E}(\mathbb{E}(\hat{\theta}_m)) = \mathbb{E}(\hat{\theta}_m)
\eeq
The second term then goes to zero are we are left with
\ber
\text{MSE} &=& \overbrace{\mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2]}^\text{variance} + \overbrace{(\mathbb{E}(\hat{\theta}_m)-\theta)^2}^\text{bias$^2$}
\eer
Shorter proof:
\ber
\text{MSE} &=& \mathbb{E}[(\hat{\theta}_m-\theta)^2] \\
&=& \mathbb{E}[\hat{\theta}^2_m - 2\theta\hat{\theta}_m + \theta^2] \\
&=& \mathbb{E}[\hat{\theta}^2_m] -2\theta\mathbb{E}(\hat{\theta}_m) + \mathbb{E}(\theta^2)
\eer
Now,
\beq
\text{var}(\hat{\theta}_m) = \mathbb{E}[\hat{\theta}^2_m] - (\mathbb{E}[\hat{\theta}^2_m])^2
\implies
\mathbb{E}[\hat{\theta}^2_m] = \text{var}(\hat{\theta}_m) + (\mathbb{E}[\hat{\theta}^2_m])^2
\eeq
Using this
\ber
\text{MSE} &=& \text{var}(\hat{\theta}_m) + (\mathbb{E}[\hat{\theta}^2_m])^2 -2\theta\mathbb{E}(\hat{\theta}_m) + \mathbb{E}(\theta^2) \\
&=& \text{var}(\hat{\theta}_m) + (\mathbb{E}[\hat{\theta}_m-\theta])^2
\eer
Which completes the proof.
\section{Bias and consistency}
Consistency
\beq
\text{P}(|\hat{\theta}_m - \theta|) \rightarrow 0 \text{ as } m \rightarrow \infty
\eeq
Bias: difference in expected value of estimator and the true value\\
Consistency $\implies$ unbiased\\
unbiased $\notimplies$ consistency
\section{Maximum likelihood estimation}
The basic idea is to choose a probability distribution paramterized by $\pmb{\theta}$, $p_{model}(\pmb{x},\pmb{\theta})$, and to find the $\pmb{\theta}$ which maximizes the probability of the observed data $\mathbb{X}$ occurring.\\
When they say
\beq
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}} \, \mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data}}}\log(p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta}))
\eeq
It is important to note that the expectation is taken over the \textit{empirical} distribution $\hat{p}_{\text{data}}$ defined over data set $\mathbb{X}=\{\pmb{x}^{(1)},\cdots,\pmb{x}^{(m)}\}$ in which each $\pmb{x}^{(i)}$ has exactly $1/m$ probability of occuring. So, finding a theta which maximizes the probability/likelihood of $\mathbb{X}=\{\pmb{x}^{(1)},\cdots,\pmb{x}^{(m)}\}$ occurring is requivalent to maximizing the expectation of
\beq
\log(p_{\text{model}}(\pmb{x},\pmb{\theta}))
\eeq
over the empirical distribution $\mathbf{x}\sim\hat{p}_{\text{data}}$.

" We can thus see maximum likelihood as an attempt to make the model dis-
tribution match the empirical distribution $\hat{p}_{\text{data}}$ Ideally, we would like to match
the true data generating distribution $p_{\text{data}}$ , but we have no direct access to this
distribution."
\section{Linear regression as Maximum Likelihood}
- Earlier we motivated linear regression as an algorithm that takes an input $\pmb{x}$ and produces an output $\hat{y}$ using ${\pmb{\theta}}^T\pmb{x}$\\
- We learned the weights $\pmb{\theta}$ by minimizing mean squared error a criterion chosen arbitrarily.\\
- Now we think of producing a conditional distribution $p(y|\pmb{x};\pmb{\theta})$ and find the parameters in this distribution that maximize the likelihood of seeing the training data $y^{(1)},\cdots,y^{(n)}$\\
-If we choose $p(y|\pmb{x};\pmb{\theta}) \sim \mathcal{N}(y|\hat{y}(\pmb{x},\pmb{\theta}),\sigma^2)$ (there is a little fudging here: $p$ which we assumed to be a probability, is now a pdf) and $\sigma$ is a constant chosen by the user\\
\beq
\mathcal{N}(y|\hat{y}(\pmb{x},\pmb{\theta}),\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(y-\hat{y}(\pmb{x},\pmb{\theta}))^2}{2\sigma^2}\Big) 
\eeq
So,
\beq
p(y^{(i)}|\pmb{x}^{(i)};\pmb{\theta}) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(y^{(i)}-\hat{y}(\pmb{x}^{(i)},\pmb{\theta}))^2}{2\sigma^2}\Big) 
\eeq
So now we want to find a $\pmb{\theta}$ that maximizes
\beq
\sum_{i=1}^{m}\log{}p(y^{(i)}|\pmb{x}^{(i)};\pmb{\theta}) = m\log\Big(\frac{1}{\sqrt{2\pi\sigma^2}}\Big) - \sum_{i=1}^{i=n}\frac{(y^{(i)}-\hat{y}(\pmb{x}^{(i)},\pmb{\theta}))^2}{2\sigma^2}
\eeq
This is the same as minimizing the linear regression error
\beq
\sum_{i=1}^{i=n}(y^{(i)}-\hat{y}(\pmb{x}^{(i)},\pmb{\theta}))^2
\eeq
- This justifies the use of MSE as a maximum likelihood estimation procedure.
\section{Properties of maximum likelihood}
- best estimator as $m\rightarrow\infty$ in terms of the rate of convergence\\
- consistent under the conditions 1) true distribution $p_{\text{data}}$ must lie within the model family $p_{\text{model}}(\cdot;\pmb{\theta})$ and $p_{\text{data}}$ must correspond to only one $\pmb{\theta}$\\
- No consistent estimator has lower mean squared error than the maximum likelihood estimator.
\section{Equation 5.68}
This seems to be an iterated application of the following identities.
Start with
\ber
P(B)  &=& \sum_i(B|A_i)P(A_i) \text{ where $A_i$ are mutually exclusive and exhaustive leads to }\\
p(x^1)&=&\int p(x^1|\pmb{\theta})p(\pmb{\theta})d\pmb{\theta}  \text{ the sum being replaced by an integral }
\eer
Then start repating in conditional space
\ber
p(x^2|x^1)&=&\int p(x^2|\pmb{\theta})p(\pmb{\theta}|x^1)d\pmb{\theta} \\
p(x^3|x^1,x^2)&=&\int p(x^3|\pmb{\theta})p(\pmb{\theta}|x^1,x^2)d\pmb{\theta} \text{ and so on }
\eer
\section{Equation 5.74}
\beq
% https://math.stackexchange.com/questions/1281454/bayes-rule-with-3-variables/1281558
p(\pmb{w}|\pmb{X},\pmb{y}) = \frac{p(\pmb{w},\pmb{X},\pmb{y})}{p(\pmb{X},\pmb{y})} = \frac{p(\pmb{y}|\pmb{X},\pmb{w})p(\pmb{X},\pmb{w})}{p(\pmb{X},\pmb{y})}
=\frac{p(\pmb{y}|\pmb{X},\pmb{w})p(\pmb{X}|\pmb{w})p(\pmb{w})}{p(\pmb{X},\pmb{y})}
\eeq
We will say $p(\pmb{X}|\pmb{w})=p(\pmb{X})$ because the data does not depend of weights. Finally we will say $p(\pmb{X})=\text{constant}$ and $p(\pmb{X},\pmb{y})=\text{constant}$
Finally we get
\beq
p(\pmb{w}|\pmb{X},\pmb{y}) \propto  p(\pmb{y}|\pmb{X},\pmb{w})p(\pmb{w})
\eeq
\section{Bayesian linear regression}
Once you have the probability $p(\pmb{w}|\pmb{X},\pmb{y})$ (equation 5.78), we can put it into the maximum-likelihood equation and and maximize it. That becaomes equation (5.79) with a slight abuse of notation $\pmb{w}\rightarrow{\pmb{\theta}}$ and $\pmb{X}\rightarrow{\pmb{x}}$ and $\pmb{y}$ being dropped.
\section{SVM Support Vector machines}
\beq
f(\pmb{x}) = \pmb{w}^T\pmb{x} + b 
\eeq
$f(\pmb{x})>0$ then positive class is present. Else negative.
\textbf{Kernel trick}
Not much of a trick, really. Expand $w$ in terms of the training data $\pmb{x}^{(i)}$. We get
\beq
f(\pmb{x})=\pmb{w}^T\pmb{x} + b = \pmb{x}^T\pmb{w} + b = b + \sum_{i=1}^m\alpha_{i}\pmb{x}^T\pmb{x}^{(i)}
\eeq
We can go one-step further and write
\beq
f(\pmb{x}) = b + \sum_{i=1}^m\alpha_{i}\phi(\pmb{x})^T\phi(\pmb{x}^{(i)})
\eeq
We can go one more step further and write
\beq
f(\pmb{x}) = b + \sum_{i=1}^m\alpha_{i}k(\pmb{x},\pmb{x}^{(i)})
\eeq
where $k$ is some kind of dot-product between $\phi(\pmb{x})$ and $\phi(\pmb{x}^{(i)})$\\
- Kernel trick allows us to consider non-linear functions of $\pmb{x}$.\\
- Allows us to make a more efficient implementation than taking the dot product of $\phi(\pmb{x})$ and $\phi(\pmb{x}^{(i)})$\\
- In many-cases computing $k(\pmb{x},\pmb{x}^{(i)})$ is tractable even when $\phi(\pmb{x})$ is intractable.\\
- \textbf{Drawback} cost is linear in the number of training examples. \textbf{Solution} learn an $\pmb{\alpha}$ vector that is mostly zeros and evaluate only over non-zero examples. The training examples associated with non-zero weights are called \textbf{support vectors}.\\
- For billions of examples constructiing $k$ is $\mathcal{O}(m^2)$
\subsection{Efficient implementation}
\url{https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f}
Let
\beq
\phi(\pmb{x}) = \begin{bmatrix} x_1^2\\\sqrt{2}x_1x_1\\x_2^2\end{bmatrix}
\eeq
Then,
\beq
\phi(\pmb{a})^T\phi(\pmb{b}) = a_1^2b_1^2 + 2a_1b_1a_2b_2 + a_2^2b_2^2 = (\pmb{a}^T\pmb{b})^2
\eeq
Obviously just evaluating the last term is much easier than evaulating the second term.
\textbf{Important} Data becomes separable in higher dimensional space.
\section{Decision trees}
- breaks input space into regions whose boundaries are typically axis aligned \\
- does not learn decision boundaries which are non-aligned with coord axes easily\\
- e.g. in 2D a decision boundary with +ve class if $x_2>x_1$ is a line at 45 degrees with coordinate axes\\
- such boundaries are not easily learned and are approximated with a stair-step function.
\section{PCA}
- learns representaton whose elements have no linear correlation with each other.\\
- first step in learning representations whose elements are statistically independent\\
\ber
\pmb{X} = \begin{bmatrix}\pmb{x^{(1)}}^T\\\vdots\\\pmb{x^{(m)}}^T\end{bmatrix} \text{ where } \pmb{x^{(i)}} \text{ is a column feature vector}\\
\pmb{X}^T\pmb{X} = \begin{bmatrix}\pmb{x^{(1)}}& \hdots & \pmb{x^{(m)}}\end{bmatrix}\begin{bmatrix}\pmb{x^{(1)}}^T\\\vdots\\\pmb{x^{(m)}}^T\end{bmatrix} = \sum_{i=1}^{m}\pmb{x^{(i)}}\pmb{x^{(i)}}^T
\eer
\section{Stochastic gradient descent}
\textbf{Insight} Gradient is an expectation! Which can be approximated using $m^{'}\ll{m}$ examples\\
as $m\rightarrow\infty$, we can show that the optimization algorithm converges before every example has been sampled. So the computational cost is $\mathcal{O}(1)$ (That's what the book says) I think it is $\mathcal{O}(m)$.
\end{document}
%%% SO Question 1
I'm trying to learn Maximum Likelihood Estimation from "Deep Learning" by Goodfellow/Bengio/Courville. 

They start with considering a set of $m$ samples $\mathbb{X}=\{\pmb{x}^{(1)},\cdots,\pmb{x}^{(m)}\}$ drawn independently from the true but unknown data generating distribution $p_{\text{data}}(\mathbf{x})$.

Then they say, Let $p_{\text{model}}(\mathbf{x},\mathbf{\theta})$ be a parametric family of probability distributions over the same space indexed by $\pmb{\theta}$. They say: In other words, $p_{\text{model}}(\pmb{x},\pmb{\theta})$ maps any configuration $\mathbf{x}$ to a real number estimating the true probability $p_{\text{data}}(\pmb{x})$.

They define the maximum likelihood estimator as 

$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}}\prod_{i=1}^{m}p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta})
$$

And then they introduce an equivalent optimization problem
$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}}\sum_{i=1}^{m}\log(p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta}))
$$
They divide by $m$ to get
$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}}\sum_{i=1}^{m}\frac{1}{m}\log(p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta})) \qquad \cdots (1)
$$
They they say, 
$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}} \mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data}}}
$$
%%% END SO Question 1


