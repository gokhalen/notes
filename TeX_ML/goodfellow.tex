\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\ddeps}{\frac{d}{d\epsilon}\Big{|}_{\rightarrow{0}}}
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bD}{{\mathbf{D}}}
\newcommand{\notimplies}{\;\not\!\!\!\implies}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{cancel}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
% for booktabs
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\begin{document}
\title{Notes from Goodfellow}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{PCA Section 2.12}
Given a decoder $\pmb{D}$, what should the encoder be? The idea is that of all the points $\pmb{c}$ that one could map $\pmb{x}$ to, we should choose the point $\pmb{c}^*$ that minimizes $\|\pmb{x}-g(\pmb{c})\|_2$. Taking derivatives we find that the minimizer satisfies $\pmb{c}^* = \pmb{D}^T\pmb{x}$. So, $\pmb{D}^T$ is our encoder.
\section{Covariance and independence Section 3.8}
\ber
\text{ indepdent variables } & & \implies \text{ zero covariance }\\
\text{ non-zero covariance } & & \implies \text{ dependent variables }\\
\text{ zero covariance }     & & \implies \text{ no linear dependence, can be non-linear dependence }\\
\text{ dependent variables } & & \implies \text{ possible zero covariance, see below}\\
\eer
$\mathrm{x} \in [-1,1]$ be a uniform random variable. Hence, $P(x)=\frac{1}{2}$. $s$ is a discrete random variable taking values $1$ or $-1$ with probability $\frac{1}{2}$. We generate a new random variable $y=sx$. Clearly, $x$ and $y$ are not independent, because $x$ completely determines the magnitude of $y$. However, $\text{Cov}(x,y)=0$. We will prove this. \\
Clearly $x$ and $s$ are independent. Hence,
\beq
\mathbb{E}(y) = \mathbb{E}(xs) = \mathbb{E}(x)\mathbb{E}(s)
\eeq
\beq
\mathbb{E}(x) = \int_{-1}^{1}P(x)xdx = \int_{-1}^{1}\frac{1}{2}xdx = \frac{1}{2}\Big[\frac{x^2}{2}\Big]_{-1}^{1} = 0
\eeq
\beq
\mathbb{E}(s) = P(s=1)1 + P(s=-1)(-1) = 0.5*1+(0.5)*(-1) = 0
\eeq
Therefore $\mathbb{E}(y)=0$
\beq
\text{Cov}(x,y) = \mathbb{E}\Big[(x-\mathbb{E}(x))(y-\mathbb{E}(y))\Big] = \mathbb{E}\Big[xy\Big] = \mathbb{E}\Big[sx^2\Big]
\eeq
Again, since $x$ and $s$ are independent
\beq
\text{Cov}(x,y) = \mathbb{E}\Big[sx^2\Big] =  \mathbb{E}(x^2)\mathbb{E}(s)=0
\eeq
$x$ and $y$ are dependent but their covariance is zero.
Let $x,y$ be independent random variables. Then
\ber
\mathbb{E}(xy) &=& \int \int P(x,y)xy dxy = \int\int P_1(x)P_2(y) xy dxdy \\
               &=& \int P_1(x)xdx \int P_2(y)ydy = \mathbb{E}(x)\mathbb{E}(y)
\eer
I think the critical part is that $P(x,y)=P_1(x)P_2(y)$ if $x,y$ are independent.
\section{ Multinomial/Multinoulli distribution: 3.9.2 }
\subsection{Bernoulli distribution}
Random variable takes value $k=1$ with prob $\theta$ and $k=0$ with prob $1-\theta$. PMF is $\theta^k(1-\theta)^{1-k}$ assuming $0^0=1$. The true mean is
\beq
\theta = 1*P(1) + 0*P(0) = \theta
\eeq
The PMF can be written as
\beq
P(k) = (\theta)^k(1-\theta)^{1-k}
\eeq
From which it can be seen that
\beq
P(0) = (1-\theta) \qquad P(1) = \theta
\eeq
\section{Expectation of a function}
The definition is (equation 3.10) in Goodfellow
\beq
\mathbb{E}(f(x)) = \int f(x) p_x(x) dx
\eeq
If we now want to change the random variable from $x$ to $y=f(x)$ we will need to find a pdf $p_y(y)$, so that we can use the formula
\beq
\mathbb{E}(y)) = \int yp_y(y)dy 
\eeq
$p_y(y) \ne p_x(f^{-1}(y))$. We change variables in the above equation from $y$ to $x$ and get
\ber
\mathbb{E}(f(x)) &= \int f(x)p_x(x)dx \\
               &= \int y p_x(f^{-1}(y)) \frac{dx}{dy} dy
\eer
In the above equation we can identify
\beq
p_y(y) = p_x(f^{-1}(y)) \frac{dx}{dy} 
\eeq
Pretty much a standard change of variables formula.
%
%
%
\subsection{Multinoulli distribution}
From \url{https://www.euanrussano.com/post/probability/multinoulli_multinomial/}.
This distribution is also called categorial distribution, since it can be used to model events with K possible outcomes. Bernoulli distribution can be seen as a specific case of Multinoulli, where the number of possible outcomes K is 2. In machine learning, the multinoullli can used to model the expected class of one sample into a set of K classes. For instance, one may want to predict to which specie  in the set  a flower belongs based on its attribute. Then species K follow a multinoulli distribution.\\
Consider the $p(x=k)$ the probability that the sample $x$ belongs to class k. Here  $x$ could be the attributes of a flower in the example above, or one side of a die in the roll of it. If the set of classes is $K \,\in \, 1,2,3,\cdots,K$ then the probability of each outcome can be written as:
\ber
p(x=1) &=& p_1\\
p(x=2) &=& p_2\\
... & &\\
p(x=K) &=& p_K
\eer
Naturally, the probabilities sum to 1.0. ($\sum_{i}^{K}p(x=i)=1.0$). Coming back to the example of flowers classification, say that for a sample  the following probabilites where obtained for each of the 3 classes.
\ber
p(x=1) &=& 0.1\\
p(x=2) &=& 0.3\\
... & &\\
p(x=K) &=& 0.6
\eer
Clearly $\sum_{i}^{K}p(x=i)=1.0$ and one would say that the sample is most probable from class 3.\\
If we consider 3 classes corresponding to $k=1,k=2,k=3$ then the PMF can be written as
\ber
P(k)=p_1^{(k-2)(k-3)}p_2^{(k-1)(k-3)}p_3^{(k-1)(k-2)}
\eer
Can extend to $n$ classes similarly.
\subsection{Multinomial distribution}
The multinomial distribution describes repeated and independent Multinoulli trials. It is a generalization of he binomial distribution, where there may be K possible outcomes (instead of binary. As an example in machine learning and NLP (natural language processing), multinomial distribution models the counts of words in a document. Similar to Multinoulli, we say that a sample $x$ may take $K$ possible outcomes, each one with prabability $p_K$ after n successive trials. The probability (pmf) of a certain (particular) outcome can be modeled using the formula:
\beq
p(x=k) = \frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots{p_k^{x_k}}
\eeq
Where $n$ is the number of trials, $x_i$ is the number of times event $i$ occurs and is the probability $p_i$ of event $i$ at each independent trial.\\
As an example, consider a problem which can take 3 outcomes at each trial. The probability of obtaining one specific outcomes can be written as:
\beq
p(x=k) = \frac{n!}{x_1!x_2!x_3!}p_1^{x_1}p_2^{x_2}p_3^{x_3}
\eeq
This can be used to model, for instance, the probability of one specific outcome on a chess tournment. Say that we want to determine what is the probability that, after 12 games, player 1 will have 7 wins, player 2 will have 2 wins and the remaining games will finish in draw. For that, suppose that the probability that Player 1 wins is 0.4, Player 2 is 0.35 and the tie has probability 0.25. Therefore we have,
\ber
n &=& 12\\
x_1 &=& 7\\
x_2 &=& 2\\
x_3 &=& 3 \\
p1  &=& 0.4\\
p2  &=& 0.35\\
p3  &=& 0.25
\eer
Replacing that in the formula shown above:
\ber
p(x=k) = \frac{12!}{7!2!3!}{0.4}^7{0.35}^2{0.25}^3 = 0.0248
\eer
\section{Equation 3.52}
$a$ and $c$ are independent given $b$
\beq
p(a,b,c) = p(a)p(b|a)p(c|b)
\eeq
$p(a,b,c)$ seems to be the probability of $a$ and $b$ and $c$ occurring. The rhs can be simplified to
\beq
p(a,b)p(c|b) = p(a,b)p(c|a,b)  \text{ because of independence}
\eeq
which is the same thing as $p(a,b,c)$.
\section{Equation 5.1}
\beq
p(\mathbf{x}) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)
\eeq
RHS =
\beq
p(x_1\cap x_2)p(x_3|x_1x_2) = p(x_1\cap x_2 \cap x_3) = p(\mathbf{x})
\eeq
\section{Biased and unbiased estimators}
Goodfellow equation (5.21) section 5.4.2
The estimator for the mean is
\beq
\hat{\theta}_m = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}
\eeq
Now,
\beq
\mathbb{E}(\hat{\theta}_m) = \frac{1}{m}\sum_{i=1}^{m}\mathbb{E}(x^{(i)})
\eeq
Now
\beq
\mathbb{E}(x^{(i)}) = 1*P(1) + 0*P(0) = \theta \text{ ... } x^{(i)} \text{ can take values 0 or 1 only. }
\eeq
So,
\beq
\mathbb{E}(\hat{\theta}_m) = \frac{1}{m}\sum_{i=1}^{m}\theta = \theta \text{ ...the true mean. Therefore this estimator is unbiased.}  
\eeq
\subsection{Biased estimator for the variance of Gaussian}
From \url{https://dawenl.github.io/files/mle_biased.pdf} Dawen Liang CMU.
The estimator
\beq
\hat{\mu}_{m} = \frac{1}{m}\sum_{i=1}^{m} x^{(i)}
\eeq
is unbiased for the mean of the Gaussian. The biased estimator for the variance of the Gaussian is
\ber
\hat{\sigma}^2_m &=& \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\hat{\mu}_m)^2 = \frac{1}{m}\Big[\sum_{i=1}^{m}(x^{(i)})^2 - 2\hat{\mu}_{m}\sum_{i=1}^{m}x^{(i)} + \hat{\mu}_m(m)\Big]\\
&=& \frac{1}{m}\Big[\sum_{i=1}^{m}(x^{(i)})^2 - m\hat{\mu}_m^2\Big] \label{eqn:biasedgaussestim}
\eer
The expected value of this estimator is
\ber
\mathbb{E}(\hat{\sigma}^2_m) = \frac{1}{m}\Big[\sum_{i=1}^{m}\mathbb{E}((x^{(i)})^2)\Big]-\mathbb{E}(\hat{\mu}_m^2)
\eer
Using $\mathbb{E}((x^{(i)})^2)=\mathbb{E}(x^2) \,\,\forall\,\,i$
\beq
\label{eqn:expvargaussestim}
\mathbb{E}(\hat{\sigma}^2_m) = \mathbb{E}(x^2) - \mathbb{E}(\hat{\mu}^2_m)
\eeq
The true variances of $x$ and $\hat{\mu}^2$ are:
\ber
\sigma^2 = \mathbb{E}(x^2) - (\mathbb{E}(x))^2 \implies (\mathbb{E}(x^2) = \sigma^2 + \mu^2 \cdots \text{because } E(x)=\mu\\
\sigma^2_{\hat{\mu}} = \mathbb{E}(\hat{\mu}^2) - \mathbb{E}(\hat{\mu}_m)^2 \implies \mathbb{E}(\hat{\mu}^2_m) =  \sigma^2_{\hat{\mu}} + \mu^2 \cdots \text{because } E(\hat{\mu}_m)=\mu
\eer
Using the above equations in (\ref{eqn:expvargaussestim}),
\beq
\label{eqn:expvargaussestim2}
\mathbb{E}(\hat{\sigma}^2_m)  = \sigma^2 - \sigma^2_{\hat{\mu}}
\eeq
But,
\ber
\sigma^2_{\hat{\mu}_m} &=& \text{var}\Big( \frac{1}{m}\sum_{i=1}^{m} x^{(i)}\Big) \\
&=& \frac{1}{m^2}\text{var}\Big(\sum_{i=1}^{m} x^{(i)}\Big) \\
&=& \frac{1}{m^2}\sum_{i=1}^{m}\text{var}(x^{(i)}) = \frac{1}{m^2}(m\text{var}(x^{(i)}))\\
&=&\frac{1}{m}\text{var}(x^{(i)})= \frac{\sigma}{m}
\eer
Using the above equation in (\ref{eqn:expvargaussestim}),
\beq
\mathbb{E}(\hat{\sigma}^2_m) = \sigma^2 - \frac{\sigma^2}{m} = \Big(1-\frac{1}{m}\Big)\sigma^2
\eeq
Since $\mathbb{E}(\hat{\sigma}^2_m)\ne \sigma^2$, equation (\ref{eqn:biasedgaussestim}) is not an unbiased estimator for the variance.
\section{Equation 5.46}
\ber
\sqrt{\text{var}\Big[\frac{1}{m}\sum_{i=1}^{m}x^{(i)}\Big]} = \sqrt{\frac{1}{m^2}\sum_{i=1}^{m}\text{var}(x^{(i)})} = \sqrt{\frac{1}{m^2}m\text{var}(x)} = \sqrt{\frac{\sigma^2}{m}} = \frac{\sigma}{\sqrt{m}}
\eer
We have used $\text{var}(x^{(i)})=\sigma^2\,\forall\,i$
\section{Deciding which algorithm is better Pg. 128}
In machine learning experiments, it is common to say that algorithm A is better than algorithm B if the upper bound of the 95\% conﬁdence interval for the error of algorithm A is less than the lower bound of the 95\% conﬁdence interval for the error of algorithm B.
\section{Equation 5.54: Proof}
See \url{https://stats.stackexchange.com/questions/123320/mse-decomposition-to-variance-and-bias-squared} and \url{https://en.wikipedia.org/wiki/Mean_squared_error#Proof_of_variance_and_bias_relationship}
We want to prove
\beq
\text{MSE} = \mathbb{E}[(\hat{\theta}_m - \theta)^2] = \text{Bias}(\hat{\theta}_{m})^2 + \text{var}(\hat{\theta}_{m})
\eeq
$\theta$ is an known, true, non-random parameter. Proof:
\ber
\text{MSE} &=& \mathbb{E}[(\hat{\theta}_m-\theta)^2] \\
&=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m)+\mathbb{E}(\hat{\theta}_m)-\theta)^2]\\
&=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2 - 2(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))(\mathbb{E}(\hat{\theta}_m)-\theta) +  (\mathbb{E}(\hat{\theta}_m)-\theta)^2]\\
&=&  \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2] - 2\mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))(\mathbb{E}(\hat{\theta}_m)-\theta)] + \mathbb{E}[(\mathbb{E}(\hat{\theta}_m)-\theta)^2]
\eer
In the second and third term,
\beq
\mathbb{E}(\hat{\theta}_m)-\theta
\eeq
is constant, because $\theta$ is a constant true parameter and $\mathbb{E}(\hat{\theta}_m)$ is no-longer random, and hence can be taken out of the expectation. The equation then becomes,
\ber
\text{MSE} &=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2] - 2(\mathbb{E}(\hat{\theta}_m)-\theta)\mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))] + (\mathbb{E}(\hat{\theta}_m)-\theta)^2 \\
&=& \mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2] - 2(\mathbb{E}(\hat{\theta}_m)-\theta)[\mathbb{E}(\hat{\theta}_m)-\mathbb{E}(\mathbb{E}(\hat{\theta}_m))] + (\mathbb{E}(\hat{\theta}_m)-\theta)^2 \\
\eer
But
\beq
\mathbb{E}(\mathbb{E}(\hat{\theta}_m)) = \mathbb{E}(\hat{\theta}_m)
\eeq
The second term then goes to zero are we are left with
\ber
\text{MSE} &=& \overbrace{\mathbb{E}[(\hat{\theta}_m-\mathbb{E}(\hat{\theta}_m))^2]}^\text{variance} + \overbrace{(\mathbb{E}(\hat{\theta}_m)-\theta)^2}^\text{bias$^2$}
\eer
Shorter proof:
\ber
\text{MSE} &=& \mathbb{E}[(\hat{\theta}_m-\theta)^2] \\
&=& \mathbb{E}[\hat{\theta}^2_m - 2\theta\hat{\theta}_m + \theta^2] \\
&=& \mathbb{E}[\hat{\theta}^2_m] -2\theta\mathbb{E}(\hat{\theta}_m) + \mathbb{E}(\theta^2)
\eer
Now,
\beq
\text{var}(\hat{\theta}_m) = \mathbb{E}[\hat{\theta}^2_m] - (\mathbb{E}[\hat{\theta}^2_m])^2
\implies
\mathbb{E}[\hat{\theta}^2_m] = \text{var}(\hat{\theta}_m) + (\mathbb{E}[\hat{\theta}^2_m])^2
\eeq
Using this
\ber
\text{MSE} &=& \text{var}(\hat{\theta}_m) + (\mathbb{E}[\hat{\theta}^2_m])^2 -2\theta\mathbb{E}(\hat{\theta}_m) + \mathbb{E}(\theta^2) \\
&=& \text{var}(\hat{\theta}_m) + (\mathbb{E}[\hat{\theta}_m-\theta])^2
\eer
Which completes the proof.
\section{Bias and consistency}
Consistency
\beq
\text{P}(|\hat{\theta}_m - \theta|) \rightarrow 0 \text{ as } m \rightarrow \infty
\eeq
Bias: difference in expected value of estimator and the true value\\
Consistency $\implies$ unbiased\\
unbiased $\notimplies$ consistency
\section{\label{sect:mle}Maximum likelihood estimation}
Equation (5.59) The basic idea is to choose a probability distribution paramterized by $\pmb{\theta}$, $p_{model}(\pmb{x},\pmb{\theta})$, and to find the $\pmb{\theta}$ which maximizes the probability of the observed data $\mathbb{X}$ occurring.\\
When they say
\beq
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}} \, \mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data}}}\log(p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta}))
\eeq
It is important to note that the expectation is taken over the \textit{empirical} distribution $\hat{p}_{\text{data}}$ defined over data set $\mathbb{X}=\{\pmb{x}^{(1)},\cdots,\pmb{x}^{(m)}\}$ in which each $\pmb{x}^{(i)}$ has exactly $1/m$ probability of occuring. So, finding a theta which maximizes the probability/likelihood of $\mathbb{X}=\{\pmb{x}^{(1)},\cdots,\pmb{x}^{(m)}\}$ occurring is requivalent to maximizing the expectation of
\beq
\log(p_{\text{model}}(\pmb{x},\pmb{\theta}))
\eeq
over the empirical distribution $\mathbf{x}\sim\hat{p}_{\text{data}}$.

" We can thus see maximum likelihood as an attempt to make the model dis-
tribution match the empirical distribution $\hat{p}_{\text{data}}$ Ideally, we would like to match
the true data generating distribution $p_{\text{data}}$ , but we have no direct access to this
distribution."
\section{Linear regression as Maximum Likelihood}
- Earlier we motivated linear regression as an algorithm that takes an input $\pmb{x}$ and produces an output $\hat{y}$ using ${\pmb{\theta}}^T\pmb{x}$\\
- We learned the weights $\pmb{\theta}$ by minimizing mean squared error a criterion chosen arbitrarily.\\
- Now we think of producing a conditional distribution $p(y|\pmb{x};\pmb{\theta})$ and find the parameters in this distribution that maximize the likelihood of seeing the training data $y^{(1)},\cdots,y^{(n)}$\\
-If we choose $p(y|\pmb{x};\pmb{\theta}) \sim \mathcal{N}(y|\hat{y}(\pmb{x},\pmb{\theta}),\sigma^2)$ (there is a little fudging here: $p$ which we assumed to be a probability, is now a pdf) and $\sigma$ is a constant chosen by the user\\
\beq
\mathcal{N}(y|\hat{y}(\pmb{x},\pmb{\theta}),\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(y-\hat{y}(\pmb{x},\pmb{\theta}))^2}{2\sigma^2}\Big) 
\eeq
So,
\beq
p(y^{(i)}|\pmb{x}^{(i)};\pmb{\theta}) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(y^{(i)}-\hat{y}(\pmb{x}^{(i)},\pmb{\theta}))^2}{2\sigma^2}\Big) 
\eeq
So now we want to find a $\pmb{\theta}$ that maximizes
\beq
\sum_{i=1}^{m}\log{}p(y^{(i)}|\pmb{x}^{(i)};\pmb{\theta}) = m\log\Big(\frac{1}{\sqrt{2\pi\sigma^2}}\Big) - \sum_{i=1}^{i=n}\frac{(y^{(i)}-\hat{y}(\pmb{x}^{(i)},\pmb{\theta}))^2}{2\sigma^2}
\eeq
This is the same as minimizing the linear regression error
\beq
\sum_{i=1}^{i=n}(y^{(i)}-\hat{y}(\pmb{x}^{(i)},\pmb{\theta}))^2
\eeq
- This justifies the use of MSE as a maximum likelihood estimation procedure.
\section{Properties of maximum likelihood}
- best estimator as $m\rightarrow\infty$ in terms of the rate of convergence\\
- consistent under the conditions 1) true distribution $p_{\text{data}}$ must lie within the model family $p_{\text{model}}(\cdot;\pmb{\theta})$ and $p_{\text{data}}$ must correspond to only one $\pmb{\theta}$\\
- No consistent estimator has lower mean squared error than the maximum likelihood estimator.
\section{Equation 5.68}
This seems to be an iterated application of the following identities.
Start with
\ber
P(B)  &=& \sum_i(B|A_i)P(A_i) \text{ where $A_i$ are mutually exclusive and exhaustive leads to }\\
p(x^1)&=&\int p(x^1|\pmb{\theta})p(\pmb{\theta})d\pmb{\theta}  \text{ the sum being replaced by an integral }
\eer
Then start repating in conditional space
\ber
p(x^2|x^1)&=&\int p(x^2|\pmb{\theta})p(\pmb{\theta}|x^1)d\pmb{\theta} \\
p(x^3|x^1,x^2)&=&\int p(x^3|\pmb{\theta})p(\pmb{\theta}|x^1,x^2)d\pmb{\theta} \text{ and so on }
\eer
\section{Equation 5.74: Bayes' theorem with multiple variables}
\url{https://math.stackexchange.com/questions/1281454/bayes-rule-with-3-variables/1281558}
I think the thing to note here is
\beq
p(\pmb{w}|\pmb{X},\pmb{y}) \text { is the same as }   p(\pmb{w}|(\pmb{X},\pmb{y})) 
\eeq
\beq
% https://math.stackexchange.com/questions/1281454/bayes-rule-with-3-variables/1281558
p(\pmb{w}|\pmb{X},\pmb{y}) = \frac{p(\pmb{w},\pmb{X},\pmb{y})}{p(\pmb{X},\pmb{y})} = \frac{p(\pmb{y}|\pmb{X},\pmb{w})p(\pmb{X},\pmb{w})}{p(\pmb{X},\pmb{y})}
=\frac{p(\pmb{y}|\pmb{X},\pmb{w})p(\pmb{X}|\pmb{w})p(\pmb{w})}{p(\pmb{X},\pmb{y})}
\eeq
We will say $p(\pmb{X}|\pmb{w})=p(\pmb{X})$ because the data does not depend of weights. Finally we will say $p(\pmb{X})=\text{constant}$ and $p(\pmb{X},\pmb{y})=\text{constant}$
Finally we get
\beq
p(\pmb{w}|\pmb{X},\pmb{y}) \propto  p(\pmb{y}|\pmb{X},\pmb{w})p(\pmb{w})
\eeq
\section{Bayesian linear regression}
Once you have the probability $p(\pmb{w}|\pmb{X},\pmb{y})$ (equation 5.78), we can put it into the maximum-likelihood equation and and maximize it. That becomes equation (5.79) with a slight abuse of notation $\pmb{w}\rightarrow{\pmb{\theta}}$ and $\pmb{X}\rightarrow{\pmb{x}}$ and $\pmb{y}$ being dropped.
\section{SVM Support Vector machines}
\beq
f(\pmb{x}) = \pmb{w}^T\pmb{x} + b 
\eeq
$f(\pmb{x})>0$ then positive class is present. Else negative.
\textbf{Kernel trick}
Not much of a trick, really. Expand $w$ in terms of the training data $\pmb{x}^{(i)}$. We get
\beq
f(\pmb{x})=\pmb{w}^T\pmb{x} + b = \pmb{x}^T\pmb{w} + b = b + \sum_{i=1}^m\alpha_{i}\pmb{x}^T\pmb{x}^{(i)}
\eeq
We can go one-step further and write
\beq
f(\pmb{x}) = b + \sum_{i=1}^m\alpha_{i}\phi(\pmb{x})^T\phi(\pmb{x}^{(i)})
\eeq
We can go one more step further and write
\beq
f(\pmb{x}) = b + \sum_{i=1}^m\alpha_{i}k(\pmb{x},\pmb{x}^{(i)})
\eeq
where $k$ is some kind of dot-product between $\phi(\pmb{x})$ and $\phi(\pmb{x}^{(i)})$\\
- Kernel trick allows us to consider non-linear functions of $\pmb{x}$.\\
- Allows us to make a more efficient implementation than taking the dot product of $\phi(\pmb{x})$ and $\phi(\pmb{x}^{(i)})$\\
- In many-cases computing $k(\pmb{x},\pmb{x}^{(i)})$ is tractable even when $\phi(\pmb{x})$ is intractable.\\
- \textbf{Drawback} cost is linear in the number of training examples. \textbf{Solution} learn an $\pmb{\alpha}$ vector that is mostly zeros and evaluate only over non-zero examples. The training examples associated with non-zero weights are called \textbf{support vectors}.\\
- For billions of examples constructiing $k$ is $\mathcal{O}(m^2)$
\subsection{Efficient implementation}
\url{https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f}
Let
\beq
\phi(\pmb{x}) = \begin{bmatrix} x_1^2\\\sqrt{2}x_1x_1\\x_2^2\end{bmatrix}
\eeq
Then,
\beq
\phi(\pmb{a})^T\phi(\pmb{b}) = a_1^2b_1^2 + 2a_1b_1a_2b_2 + a_2^2b_2^2 = (\pmb{a}^T\pmb{b})^2
\eeq
Obviously just evaluating the last term is much easier than evaulating the second term.
\textbf{Important} Data becomes separable in higher dimensional space.
\section{Decision trees}
- breaks input space into regions whose boundaries are typically axis aligned \\
- does not learn decision boundaries which are non-aligned with coord axes easily\\
- e.g. in 2D a decision boundary with +ve class if $x_2>x_1$ is a line at 45 degrees with coordinate axes\\
- such boundaries are not easily learned and are approximated with a stair-step function.
\section{PCA}
- learns representaton whose elements have no linear correlation with each other.\\
- first step in learning representations whose elements are statistically independent\\
\ber
\pmb{X} = \begin{bmatrix}\pmb{x^{(1)}}^T\\\vdots\\\pmb{x^{(m)}}^T\end{bmatrix} \text{ where } \pmb{x^{(i)}} \text{ is a column feature vector}\\
\pmb{X}^T\pmb{X} = \begin{bmatrix}\pmb{x^{(1)}}& \hdots & \pmb{x^{(m)}}\end{bmatrix}\begin{bmatrix}\pmb{x^{(1)}}^T\\\vdots\\\pmb{x^{(m)}}^T\end{bmatrix} = \sum_{i=1}^{m}\pmb{x^{(i)}}\pmb{x^{(i)}}^T
  \eer
\section{Stochastic gradient descent}
\textbf{Insight} Gradient is an expectation! Which can be approximated using $m^{'}\ll{m}$ examples\\
as $m\rightarrow\infty$, we can show that the optimization algorithm converges before every example has been sampled. So the computational cost is $\mathcal{O}(1)$ (That's what the book says) I think it is $\mathcal{O}(m)$.
%
%
%
\section{Mean and median}
\subsection{Mean: correct}
The derivation is from \url{https://math.stackexchange.com/questions/4256801/find-arg-min-f-mathbbe-xyy-fx2-where-x-y-are-random-variable/4256809#4256809}
\begin{align}
\mathsf{E}[(y-f(x))^2]&=\mathsf{E}[(y-\mathsf{E}[y\mid x]+\mathsf{E}[y\mid x]-f(x))^2] \\
&=\mathsf{E}[(y-\mathsf{E}[y\mid x])^2]+\mathsf{E}[(\mathsf{E}[y\mid x]-f(x))^2] \\
&\quad+2\mathsf{E}[(y-\mathsf{E}[y\mid x])(\mathsf{E}[y\mid x]-f(x))] \\
&=\mathsf{E}[(y-\mathsf{E}[y\mid x])^2]+\mathsf{E}[(\mathsf{E}[y\mid x]-f(x))^2] \\
&\ge \mathsf{E}[(y-\mathsf{E}[y\mid x])^2]. 
\end{align}
The tricky part is proving $\mathsf{E}[(y-\mathsf{E}[y\mid x])(\mathsf{E}[y\mid x]-f(x))]=0$. This can be justified by holding $x$ constant and pulling out $(\mathsf{E}[y\mid x]-f(x))$. Basically, we are saying given an $x$ what is the $f$ that will minimize $\mathsf{E}[(y-f(x))^2]$. Then we are left with
\beq
\mathsf{E}[(y-\mathsf{E}[y\mid x]) = \mathsf{E}(y) - \mathsf{E}(\mathsf{E}[y\mid x])) = \mathsf{E}(y) - \mathsf{E}(y) = 0
\eeq
Since
\beq
\mathsf{E}[(y-f(x))^2] \ge \mathsf{E}[(y-\mathsf{E}[y\mid x])^2]
\eeq
The minimum is attained at
\beq
f(x) = \mathsf{E}[y\mid x]
\eeq
See \url{https://math.stackexchange.com/questions/1915324/find-fx-that-minimizes-ey-fx2x/1915328#1915328}. Hehas posted explanation there too.  
\subsection{Mean: instructive but wrong}
This is wrong because we have wrongly assumed that $\pmb{y}$ comes from the empirical distribution $\hat{p}_{data}$, whereas it is actually from $p_{data}$.
\ber
f^{*} &=& \text{arg } \underset{f}{\text{min}}\, {\mathbb{E}}_{\pmb{y}\sim{p_{data}}}\|{\pmb{y}}-f(\pmb{x})\|^2 \\
     &=& \text{arg } \underset{f}{\text{min}} \frac{1}{m}\sum_{i=1}^{m}\|{\pmb{y}^{(i)}}-f(\pmb{x})\|^2
\eer
Take variations in $f$ and set them to zero to get
\beq
\sum_{i=1}^{m}(\pmb{y}^{(i)}-f,\delta{f}) = 0 \text{ where, } (\cdot,\cdot) \text{ is the inner-product corrresponding to }\|\cdot\|
\eeq
Taking summation and noting that the above equation is true $\forall\,\delta{f}$
\beq
(\sum_{i=1}^{m}\pmb{y}^{(i)}-mf),\delta{f}) = 0 \, \implies\,f(\pmb{x})=\frac{1}{m}\sum_{i=1}^{m}\pmb{y}^{(i)} = \mathbb{E}_{y\sim{p_{data(\pmb{y}|\pmb{x})}}}[\pmb{y}]
\eeq
Where we have used arguments similar to Section (\ref{sect:mle})
\subsection{Median}
\url{https://math.stackexchange.com/questions/85448/why-does-the-median-minimize-ex-c} The $c$ in the following derivation corresponds to $f(x)$ in our work\\


Let $f$ be the pdf and let $J(c) = E(|X-c|)$. We want to maximize $J(c)$. Note that $E(|X-c|) = \int_{\mathbb{R}} |x-c| f(x) dx = \int_{-\infty}^{c} (c-x) f(x) dx  + \int_c^{\infty} (x-c) f(x) dx.$

To find the maximum, set $\frac{dJ}{dc} = 0$. Hence, we get that,
\begin{align}
\frac{dJ}{dc} & = (c-x)f(x) | _{x=c} + \int_{-\infty}^{c} f(x) dx + (x-c)f(x) | _{x=c} - \int_c^{\infty} f(x) dx\\
& = \int_{-\infty}^{c} f(x) dx - \int_c^{\infty} f(x) dx = 0
\end{align}


Hence, we get that $c$ is such that $$\int_{-\infty}^{c} f(x) dx = \int_c^{\infty} f(x) dx$$ i.e. $$P(X \leq c) = P(X > c).$$

However, we also know that $P(X \leq c) + P(X > c) = 1$. Hence, we get that $$P(X \leq c) = P(X > c) = \frac12.$$

**EDIT**

When $X$ doesn't have a density, all you need to do is to make use of integration by parts. We get that $$\displaystyle \int_{-\infty}^{c} (c-x) dP(x) = \lim_{y \rightarrow -\infty} (c-y) P(y) + \displaystyle \int_{c}^{\infty} P(x) dx.$$ Similarly, we also get that $$\displaystyle \int_{c}^{\infty} (x-c) dP(x) = \lim_{y \rightarrow \infty} (y-c) P(y) - \displaystyle \int_{c}^{\infty} P(x) dx.$$

\subsection{Median: Instructive but wrong}
\ber
f^{*} &=& \text{arg } \underset{f}{\text{min}}\, {\mathbb{E}}_{\pmb{y}\sim{p_{data}}}\|{\pmb{y}}-f(\pmb{x})\|_1 \
\eer
yields a function which predicts the median value of $\pmb{y}$ for each $\pmb{x}$. Treatment is as in \url{https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-ell-1-norm}. We restrict ourselves to scalar $\pmb{y}$, and note that the treatment will work as long as an appropriate \text{sign} function is defined for vectors. For scalars, the optimization problem is
\ber
f^{*} &=& \text{arg } \underset{f}{\text{min}}\, {\mathbb{E}}_{\pmb{y}\sim{p_{data}}}\|{\pmb{y}}-f(\pmb{x})\| \text{ where } \|\cdot\| \text{ is the absolute value}
\eer
First we note that
\beq
\dd{|x|}{x} = \text{sign}(x)
\eeq
Our optimization problem is
\beq
\label{eqn:optprob}
f^{*} = \text{arg } \underset{f}{\text{min}}\, \frac{1}{m}\sum_{i=1}^{m}|y^{i}-f|
\eeq
We need to take variations in $f$ and set them to zero.
\ber
\text{Let }\pi^{i} &=& |y^{i}-f| \\
\delta_f \pi^{i} &=& \ddeps|y^{i}-f-\epsilon\delta{f}|\\
&=& \ddeps \,\, \Big{[} |y^{i}-f| + \dd{|y^{i}-f|}{f}(-\epsilon\delta{f}) \cdots \text{ higher order terms }\Big{]} \text{ by chain rule } \\
&=& \ddeps \,\, \Big{[}  \dd{|y^{i}-f|}{f}(-\epsilon\delta{f})\Big{]}\\
&=& \ddeps \,\, \Big{[} \dd{|y^{i}-f|}{(y^{i}-f)}\dd{y^{i}-f}{f}(-\epsilon\delta{f})\Big{]}\\
&=&  \ddeps \,\, \text{sign}(y^{i}-f)(-1)(-\epsilon\delta{f})\Big{]}\\
&=& \text{sign}(y^{i}-f)\delta{f}  
\eer
Taking variations in $f$ in equation (\ref{eqn:optprob}) and setting them to zero gives
\beq
(\sum_{i=1}^{m}\text{sign}(y^{i}-f))\delta{f} \,\, \forall\, \delta{f}
\eeq
This can happen only if the number of terms with +ve sign in the above equation are equal to the number of terms with negative sign. i.e. if $f$ is the median.
\subsection{Median: Generalization to vector functions}
The treatment is the same as above. Consider $\pmb{f}$ and the data $\pmb{y}$ to be in $\mathbb{R}^2$. Then the optimization problem is
\beq
f^{*} = \text{arg } \underset{f}{\text{min}}\, \frac{1}{m}\sum_{i=1}^{m}\|\pmb{y}^{i}-\pmb{f}\|_1
\eeq
Using the definition of the $\|\cdot\|_1$ we get
\beq
f^{*} = \text{arg } \underset{f}{\text{min}}\, \frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{2}\|{y}^{i}_j-f_j\|_1
\eeq
Using the definition of $\|\cdot\|_1$ we have
\beq
f^{*} = \frac{1}{m}\text{arg } \underset{f}{\text{min}}\, |{y}^{i}_1-f_1|_1 + |{y}^{i}_2-f_2|_1
\eeq
Take variations in $f$, in both its components. Since these variations are arbitrary, we can choose in sequence $\delta\pmb{f}=[\delta{f}_1,0]$ and then $\delta\pmb{f}=[0,\delta{f}_2]$. This has the effect of reducing the above optimization problem in which variations in both directions were present to one in which variations in only one direction are present, rendering it the same as the scalar case. So, in the case of vector functions, it finds the median in all directions. Specifically, in 2D it is going to predict an answer which has the same number of examples to its left and right, and, up and down. 
%
%
%
\section{Sigmoid eqn 6.20-6.23}
\url{https://stats.stackexchange.com/questions/253273/inverse-logit-sigmoid-algebraic-manipulations-in-ian-goodfellows-deep-learning}
The point to note that
\beq
\sigma((2y-1)z) \ne \frac{\exp(yz)}{\sum_{y^{'}=0}^{1}\exp(y^{'}z)} \text{ for all values of y}
\eeq
The equality holds only for $y=0$ or $y=1$, which are the values that $y$ is allowed to take for binary classification, which is the primary use of sigmoid. The $z$ that goes into $\sigma$ is called a logit.

\section{Sigmoid two class classification properties}
See table (\ref{tab:sigmoid}). Summarizes discussion in Goodfellow after equation (6.26). The important thing is the use of the sigmoid to evaluate probabilities. $P(0)=P(y=0)=\sigma(-z)$ and similarly for $P(1)$.
\begin{table}\centering
  \ra{1.3}
  \begin{tabular}{ccc}
    \toprule
    \multicolumn{3}{c}{Correct answer $y=0$}\\
    \midrule
    {Quantities} &  {$z$ +ve and large }  & {$z$ -ve and large}\\
    %&                                  &  \\
    \midrule
    $P(y=0)=\sigma(-z)$  &  $0$ (wrong)  &  $1$ (correct)\\
    loss = $\zeta$(z)    &  (large)      &  (small)\\
    gradient = $\dd{\zeta}{z}=\sigma(z)$ & (large)  & (small)\\
    \toprule
    \multicolumn{3}{c}{Correct answer $y=1$}\\
    \midrule
    $P(y=1)=\sigma(z)$  &  $1$ (correct)  &  $0$ (wrong)\\
    loss = $\zeta$(-z)    &  (small)      &  (large)\\
    gradient = $\dd{\zeta}{z}=-\sigma(-z)$ & (small)  & (large)\\
    \midrule
  \end{tabular}
  \caption{\label{tab:sigmoid} Two class classification properties. When we have the correct ans}  
\end{table}
\section{Softmax}
\subsection{Numerical stability}
\url{https://stackoverflow.com/questions/42599498/numercially-stable-softmax}
\section{Binary function for classification: pg 199 }
A binary function is one whose output is either 0 or 1. i.e. Range is {0,1}. Recall the basic definition of a function. Relation between two sets. Or a mapping from one set to another. If the domain has $n$ elements and the range has $m$ elements then there are $m^n$ functions from domain to range. This is because each element in domain can take the $m$ values  and there are $n$ such elements. Now, in the context of binary classification, we are interested in the case where $m=2$. Furthermore, if our feature vectors are consisting only of 0s and 1s and have length $n$ i.e. they are in $\in\{0,1\}^n$ then there are at most $2^n$ feature vectors. A training set consists of $n_1$ of these vectors mapped to $1$ and the remaining $2^n-n_1$ mapped to zero. This mapping is just one function of the total possible $2^{2^n}$ functions. This illustrates how hard the problem of binary classification is, not to say anything of $k$ class classification.
\end{document}
%%% SO Question 1
I'm trying to learn Maximum Likelihood Estimation from "Deep Learning" by Goodfellow/Bengio/Courville. 

They start with considering a set of $m$ samples $\mathbb{X}=\{\pmb{x}^{(1)},\cdots,\pmb{x}^{(m)}\}$ drawn independently from the true but unknown data generating distribution $p_{\text{data}}(\mathbf{x})$.

Then they say, Let $p_{\text{model}}(\mathbf{x},\mathbf{\theta})$ be a parametric family of probability distributions over the same space indexed by $\pmb{\theta}$. They say: In other words, $p_{\text{model}}(\pmb{x},\pmb{\theta})$ maps any configuration $\mathbf{x}$ to a real number estimating the true probability $p_{\text{data}}(\pmb{x})$.

They define the maximum likelihood estimator as 

$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}}\prod_{i=1}^{m}p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta})
$$

And then they introduce an equivalent optimization problem
$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}}\sum_{i=1}^{m}\log(p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta}))
$$
They divide by $m$ to get
$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}}\sum_{i=1}^{m}\frac{1}{m}\log(p_{\text{model}}(\pmb{x}^{(i)},\pmb{\theta})) \qquad \cdots (1)
$$
They they say, 
$$
\mathbf{\theta}_{\text{ML}} = \text{arg } \underset{\pmb{\theta}}{\text{max}} \mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data}}}
$$
%%% END SO Question 1


