\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bD}{{\mathbf{D}}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{tikz}
\usepackage{cancel}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
\begin{document}
\title{Notes from Goodfellow}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{PCA Section 2.12}
Given a decoder $\pmb{D}$, what should the encoder be? The idea is that of all the points $\pmb{c}$ that one could map $\pmb{x}$ to, we should choose the point $\pmb{c}^*$ that minimizes $\|\pmb{x}-g(\pmb{c})\|_2$. Taking derivatives we find that the minimizer satisfies $\pmb{c}^* = \pmb{D}^T\pmb{x}$. So, $\pmb{D}^T$ is our encoder.
\section{Covariance and independence Section 3.8}
\ber
\text{ indepdent variables } & & \implies \text{ zero covariance }\\
\text{ non-zero covariance } & & \implies \text{ dependent variables }\\
\text{ zero covariance }     & & \implies \text{ no linear dependence, can be non-linear dependence }\\
\text{ dependent variables } & & \implies \text{ possible zero covariance, see below}\\
\eer
$\mathrm{x} \in [-1,1]$ be a uniform random variable. Hence, $P(x)=\frac{1}{2}$. $s$ is a discrete random variable taking values $1$ or $-1$ with probability $\frac{1}{2}$. We generate a new random variable $y=sx$. Clearly, $x$ and $y$ are not independent, because $x$ completely determines the magnitude of $y$. However, $\text{Cov}(x,y)=0$. We will prove this. \\
Clearly $x$ and $s$ are independent. Hence,
\beq
\mathbb{E}(y) = \mathbb{E}(xs) = \mathbb{E}(x)\mathbb{E}(s)
\eeq
\beq
\mathbb{E}(x) = \int_{-1}^{1}P(x)xdx = \int_{-1}^{1}\frac{1}{2}xdx = \frac{1}{2}\Big[\frac{x^2}{2}\Big]_{-1}^{1} = 0
\eeq
\beq
\mathbb{E}(s) = P(s=1)1 + P(s=-1)(-1) = 0.5*1+(0.5)*(-1) = 0
\eeq
Therefore $\mathbb{E}(y)=0$
\beq
\text{Cov}(x,y) = \mathbb{E}\Big[(x-\mathbb{E}(x))(y-\mathbb{E}(y))\Big] = \mathbb{E}\Big[xy\Big] = \mathbb{E}\Big[sx^2\Big]
\eeq
Again, since $x$ and $s$ are independent
\beq
\text{Cov}(x,y) = \mathbb{E}\Big[sx^2\Big] =  \mathbb{E}(x^2)\mathbb{E}(s)=0
\eeq
$x$ and $y$ are dependent but their covariance is zero.
\section{ Multinomial/Multinoulli distribution: 3.9.2 }
From \url{https://www.euanrussano.com/post/probability/multinoulli_multinomial/}.
\subsection{Multinoulli distribution}
This distribution is also called categorial distribution, since it can be used to model events with K possible outcomes. Bernoulli distribution can be seen as a specific case of Multinoulli, where the number of possible outcomes K is 2. In machine learning, the multinoullli can used to model the expected class of one sample into a set of K classes. For instance, one may want to predict to which specie  in the set  a flower belongs based on its attribute. Then species K follow a multinoulli distribution.\\
Consider the $p(x=k)$ the probability that the sample $x$ belongs to class k. Here  $x$ could be the attributes of a flower in the example above, or one side of a die in the roll of it. If the set of classes is $K \,\in \, 1,2,3,\cdots,K$ then the probability of each outcome can be written as:
\ber
p(x=1) &=& p_1\\
p(x=2) &=& p_2\\
... & &\\
p(x=K) &=& p_K
\eer
Naturally, the probabilities sum to 1.0. ($\sum_{i}^{K}p(x=i)=1.0$). Coming back to the example of flowers classification, say that for a sample  the following probabilites where obtained for each of the 3 classes.
\ber
p(x=1) &=& 0.1\\
p(x=2) &=& 0.3\\
... & &\\
p(x=K) &=& 0.6
\eer
Clearly $\sum_{i}^{K}p(x=i)=1.0$ and one would say that the sample is most probable from class 3.
\subsection{Multinomial distribution}
The multinomial distribution describes repeated and independent Multinoulli trials. It is a generalization of he binomial distribution, where there may be K possible outcomes (instead of binary. As an example in machine learning and NLP (natural language processing), multinomial distribution models the counts of words in a document. Similar to Multinoulli, we say that a sample $x$ may take $K$ possible outcomes, each one with prabability $p_K$ after n successive trials. The probability (pmf) of a certain (particular) outcome can be modeled using the formula:
\beq
p(x=k) = \frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots{p_k^{x_k}}
\eeq
Where $n$ is the number of trials, $x_i$ is the number of times event $i$ occurs and is the probability $p_i$ of event $i$ at each independent trial.\\
As an example, consider a problem which can take 3 outcomes at each trial. The probability of obtaining one specific outcomes can be written as:
\beq
p(x=k) = \frac{n!}{x_1!x_2!x_3!}p_1^{x_1}p_2^{x_2}p_3^{x_3}
\eeq
This can be used to model, for instance, the probability of one specific outcome on a chess tournment. Say that we want to determine what is the probability that, after 12 games, player 1 will have 7 wins, player 2 will have 2 wins and the remaining games will finish in draw. For that, suppose that the probability that Player 1 wins is 0.4, Player 2 is 0.35 and the tie has probability 0.25. Therefore we have,
\ber
n &=& 12\\
x_1 &=& 7\\
x_2 &=& 2\\
x_3 &=& 3 \\
p1  &=& 0.4\\
p2  &=& 0.35\\
p3  &=& 0.25
\eer
Replacing that in the formula shown above:
\ber
p(x=k) = \frac{12!}{7!2!3!}{0.4}^7{0.35}^2{0.25}^3 = 0.0248
\eer
\end{document}

