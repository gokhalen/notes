\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bPi}{\mathbf{\Pi}}
\newcommand{\bQ}{{\mathbf{Q}}}
\newcommand{\bq}{{\mathbf{q}}}
\newcommand{\bK}{{\mathbf{K}}}
\newcommand{\bU}{{\mathbf{U}}}
\newcommand{\bS}{{\mathbf{S}}}
\newcommand{\bV}{{\mathbf{V}}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{tikz}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
\begin{document}
\title{PCA and POD and SVD}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Principal Component Analysis}
According to Ng, we use PCA for data compression or visualization. As per Andrew Ng's notes, PCA goes like the following. First take the features $x^{(i)} \in \mathbb{R}^n$ which are assumed to have zero mean and and also perhaps scaled by the variance/standard deviation and construct the matrix

\ber
\label{eqn:discretexxt}
\Sigma = \frac{1}{m}\sum_{i=0}^{m}x^{(i)}{x^{(i)}}^{T}
\eer
%
SVD of $\Sigma$ gives
\beq
\Sigma = USV^{T} \qquad U,S,V \in \mathbb{R}^{n\times{n}}
\eeq
Pick the first $k$ columns of $U$:
\beq
\hat{U} = U[:,0:k]  \qquad \hat{U} \in \mathbb{R}^{n\times{k}}
\eeq
We can view $U$ containing information about which combination of features is most significant.
Define new variables:
\beq
\label{eqn:ngproj}
z^{(i)}   = {\hat{U}}^{T}x^{(i)} \qquad z^{(i)} \in \mathbb{R}^{k},\,\, {\hat{U}}^{T} \in \mathbb{R}^{k\times{n}}
\eeq
Now training data becomes $(z^{(0)},y^{(0)}), (z^{(1)},y^{(1)}), \cdots, (z^{(m)},y^{(m)})$. If needed, the original variables can be recovered using
\beq
x^{(i)} = \hat{U}z^{(i)}
\eeq
\textbf{Output of PCA (z) depends on how the eigenvectors/sigularvectors are normalized! You can flip the sign of a singular vector. It will still remain a singular vector with the same eigenvalue, but z will change! May 5 2022: This is probably because, symmetry of $\Sigma$ implies $V=U$ and we can replace $V$ and $U$ by $-V$ and $-U$ respectively.} 
%
%
%
\subsection{Aside} 
The matrix $\Sigma$ is symmetric-positive-semi-definite. To see this define:
\beq
a_{pq}^{(i)} = x^{(i)}_{p}{x^{(i)}_{q}}
\eeq
We have:
\ber
y_{p}a_{pq}^{(i)}y_{q} &=& y_{p}x^{(i)}_{p}{x^{(i)}_{q}}y_{q}  \\
&=& (y_{c}x^{(i)}_{c})({x^{(i)}_{c}}y_{c}) \text{ ... switching dummy inidces to }c\\
&=& (y_{c}x^{(i)}_{c})^2 \ge 0
\eer
Since, $\Sigma$ is a sum of all $a^{i}$, it is also symmetric-positive-semi-definite.
\section{Model reduction in mechanics}
We are typically interested in solving the system
\beq
\label{eqn:kxeqf}
Kx = f \qquad \text{where } K\in\mathbb{R}^{n\times{n}} \text{ and } x,f \in \mathbb{R}^n
\eeq
$K$ is symmetric positive definite with distinct eigenvalues. We typically solve for $k$ eigenvectors of $K$ with $k \ll n$. We construct a matrix $\phi$ whose columns are the $k$ eigenvectors of $K$. So $\phi \in \mathbb{R}^{n\times{k}}$. We define new variables $y$
\beq
x = \phi{y} \qquad \text{ or equivalently } y = \phi^Tx
\eeq
This is possible because the eigenvectors of symmetric positive definite matrices with distinct eigenvalues are orthogoal. This gives
\beq
\phi^Tx = \phi^T\phi y = y \qquad \text{ since } \phi^T\phi = \mathbb{I}^{k\times{k}}, \qquad \mathbb{I} \text{ is the identity matrix.}
\eeq
Using the above equation in equation (\ref{eqn:kxeqf}) we get
\beq
K\phi{y} = f
\eeq
Multiplying by $\phi^T$ gives a square $k\times{k}$ system
\beq
\phi^TK\phi{y} = \phi^Tf
\eeq
\subsection{Aside 1}
Saying $x=\phi{y}$ is equivalent to saying $x=y_{1}\phi_{1}+\cdots+y_{k}\phi_{k}$ where $\phi_m$ is the $m^{\text{th}}$ column of $\phi$, i.e. the $m^{\text{th}}$ eigenvector and $y_m$ is the $m^{\text{th}}$ component of $y$. This is the conventional expansion of $x$ using the significant eigenfunctions of $K$. To see this, consider the following small example
\ber
\begin{pmatrix}
  \phi_{00} & \phi_{01} \\
  \phi_{10} & \phi_{11} \\
  \phi_{20} & \phi_{21} \\
  \phi_{30} & \phi_{31} \\
\end{pmatrix}
\begin{pmatrix}
  y_0 \\
  y_1
\end{pmatrix}
=\begin{pmatrix}
  \phi_{00}y_0 + \phi_{01}y_1 \\
  \phi_{10}y_0 + \phi_{11}y_1 \\
  \phi_{20}y_0 + \phi_{21}y_1 \\
  \phi_{30}y_0 + \phi_{31}y_1\\
\end{pmatrix}
=y_0\begin{pmatrix}
  \phi_{00}  \\
  \phi_{10}  \\
  \phi_{20}  \\
  \phi_{30}  \\
\end{pmatrix}
+
y_1\begin{pmatrix}
  \phi_{01}  \\
  \phi_{11}  \\
  \phi_{21}  \\
  \phi_{31}  \\
\end{pmatrix}
\eer
\subsection{Aside 2}
Let $K\in\mathbb{R}^{n\times{n}} $ be a symmetric positive definite matrix with $n$ distinct eigenvalues. Let $x$ and $y$ be eigenvectors associated with $\lambda_x$ and $\lambda_y$. We have
\beq
Kx=\lambda_xx \implies y^TKx = \lambda_xy^Tx
\eeq
But,
\beq
y^TKx = (Ky)^Tx = (\lambda_yy)^Tx = \lambda_yy^Tx
\eeq
We must have
\beq
\lambda_xy^Tx = \lambda_yy^Tx
\eeq
Since $\lambda_x\ne\lambda_y$, we must have $y^Tx=0$.
This gives rise to the identity we have used before:
\beq
\phi^T\phi = \mathbb{I}^{k\times{k}}
\eeq
%\section{POD}
%Why do we seek a projector? We seek a projector so that we can project the ODEs onto a lower dimensional basis, resulting in fewer odes to be marched in time. Suppose we are solving

%\ber
%\dd{w(t)}{t} = f(w(t),t) \qquad \text{ where } w \in \mathbb{R}^{N}
%\eer

%Once we have a projector say $\Sigma \in \mathbb{R}^{k\times{N}}$, we can multiply both sides of the above equation by $\Sigma$ to get a lower dimensional system. The singular value decomposition and the method of snapshots essentially provide a way to compute the projector $\Sigma$.

\section{Proper Orthogonal Decomposition}
This treatment is from \url{https://web.stanford.edu/group/frg/course_work/CME345/CA-CME345-Ch4.pdf}. Consider the following system of odes which can come from a discretization in space.

\beq
\label{eqn:podgovern}
\dd{\bw(t)}{t} = \bff(\bw(t),t)
\eeq

Here $\bw,\bff \in \mathbb{R}^{N}$, where $N$ is a large number. Integrating this system in time takes large computational resources. So we want to convert it to a smaller system. This can be done by multiplying with an orthogonal matrix $\bPi\in\mathbb{R}^{k\times{N}}$ where $k\ll N$. Note that when a matrix multiplies a vector, we get a new vector we get a new vector whose components are the original vector multiplied by the rows of the matrix. If the rows have unit magnitude and are orthogonal to each other, then the new vector is the projection of the original vector E.g. Let $\bQ\in\mathbb{R}^{k\times{N}}$ be a matrix whose rows are orthogonal to each other.
\beq
\bQ = \begin{pmatrix} \bq_1^T\\ \bq_2^T\\ .\\ . \\ \bq_k^T\end{pmatrix} \text{ then }
\bQ\bw = \begin{pmatrix} \bq_1^T\bw\\ \bq_2^T\bw\\ .\\ . \\ \bq_k^T\bw\end{pmatrix}
\eeq
We intend to multiply equation (\ref{eqn:podgovern}) with $\bPi$ to get a lower dimensional system. That is 
\beq
\label{eqn:podgovernproj}
\dd{\bPi\bw(t)}{t} = \bPi\bff(\bw(t),t)
\eeq
The projection reduces the dimension of the system from $N$ to $k$, because $\bPi\bw,\bPi\bff \in \mathbb{R}^{k}$.\\

The question now is: how to find the projector $\bPi$?. We want $\bPi$ such that the following quantity $J(\bPi)$ is minimized.

\beq
\label{eqn:jmin}
J(\bPi)=\int_{0}^{T} \|\bw(t)-\bPi\bw(t)\|_2^2
\eeq

The following theorem comes to the rescue and gives us a way to compute $\bPi$. Define $\hat{\bK}\in \mathbb{R}^{N\times{N}}$ be

\beq
\label{eqn:continuousxxt}
\hat{\bK} = \int_{0}^{T} \bw(t)\bw(t)^T dt
\eeq

Let $\hat{\lambda}_1,\hat{\lambda}_2,\hat{\lambda}_3,\cdots,\hat{\lambda}_N \ge 0$ be its eigenvalues and $\hat{\phi}_i$ be the associated eigenvectors. Then, if we are looking for an operator $\bPi$ of rank $k$ that minimizes equation (\ref{eqn:jmin}), then the range of the operator is spanned by the eigenvectors $\hat{\phi}_1,\hat{\phi}_2,\cdots,\hat{\phi}_k$ asssociated with the eigenvalues $\hat{\lambda}_1,\hat{\lambda}_2,\hat{\lambda}_3,\cdots,\hat{\lambda}_k \ge 0$. This means that we can take the SVD of $\hat{\bK}=\bU\bS\bV^T$ and take the first $k$ columns of $\bU$ to get $\bPi$. But since $\hat{\bK}$ is a large and dense matrix, the SVD of it is intractable. Farhat dicusses ways to get around this, using the method of snapshots and other tricks.

\section{Connection with PCA as used in Data Science}
We want to find an orthogonal projector us that minimizes a discrete version of equation (\ref{eqn:jmin}). This equation is given by
\beq
J = \sum_{i=0}^{m}\|x^{(i)}-\bPi{x^{(i)}}\|_2^2
\eeq
I'm pretty sure the above equation is not entirely corect in the context of data science. I think it should be
\beq
\label{eqn:minimizedifference}
J = \sum_{i=0}^{m}\|x^{(i)}-\hat{U}\hat{U}^T{x^{(i)}}\|_2^2
\eeq
And we find the optimal projector $U$ of rank $k$.The rest of the theory is pretty much the same as POD theory. Equation (\ref{eqn:discretexxt}) is a discrete analog of (\ref{eqn:continuousxxt}).
%
%
%
\section{SVD of data}
From \url{https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/}
\ber
% 2x2 grid of matrices
\begin{matrix}
% 1x1 matrix                    1x2 matrix
\begin{matrix} \end{matrix} & \begin{matrix} \text{Aisha} & \text{Bob} & \text{Chandrika} \end{matrix}\\
% 2x1 matrix  
\begin{matrix}
  \text{Up}\\
  \text{Skyfall}\\
  \text{Thor}\\
  \text{Amelie}\\
  \text{Snatch}\\
  \text{Casablanca}\\
  \text{Bridesmaids}\\
  \text{Grease}\\
\end{matrix}&
\begin{pmatrix}
  2 \hspace{0.7cm} & 5 \hspace{0.8cm} & 3 \hspace{1cm}\\
  1 \hspace{0.7cm} & 2 \hspace{0.8cm} & 1 \hspace{1cm}\\
  4 \hspace{0.7cm} & 1 \hspace{0.8cm} & 1 \hspace{1cm}\\
  3 \hspace{0.7cm} & 5 \hspace{0.8cm} & 2 \hspace{1cm}\\
  5 \hspace{0.7cm} & 3 \hspace{0.8cm} & 1 \hspace{1cm}\\
  4 \hspace{0.7cm} & 5 \hspace{0.8cm} & 5 \hspace{1cm}\\
  2 \hspace{0.7cm} & 4 \hspace{0.8cm} & 2 \hspace{1cm}\\
  2 \hspace{0.7cm} & 2 \hspace{0.8cm} & 5 \hspace{1cm}\\
\end{pmatrix}
\end{matrix}
\eer
We want to come up with ratings of these movies for a new user. We want to factor this matrix $A\in\mathbb{R}^{8\times{3}}$ using the SVD. That is
\beq
A^{8\times{3}} = U^{8\times{8}}\Sigma^{8\times{3}}{V^T}^{3\times{3}}
\eeq
The SVD can be decomposed as
\beq
A = \sum_i u^i\sigma_i{v^i}^T
\eeq
Where $u^i,v^i$ are column vectors of $U$ and $V$. Here, $A$ can be used to predict movie ratings for a new user. We shall now see what $u^i$ and $v^i$ mean. If have a new user which we can express as the weighted sum of Aisha, Bob, Chandrika e.g.
\beq
x_{user} =
\begin{pmatrix}0.9,0.05,0.05\end{pmatrix}
  \eeq
  
  Then we can predict movie-ratings for this user using $Ax_{user}$. Suppose now that we have too many raters and want to reduce the number of raters to 1. We can do this by approximating $A$ using the following formula.
  \beq
  \bar{A} = u^1\sigma_1{v^1}^T
  \eeq
  Here, $u^1$ is a new fictitious rater who rates the movies in a certain way, so that there is the least difference in $Ax_{user}-\bar{A}x_{user}$. Since $\|u^1\|=1$, this corresponds to a certain direction in ratings space ($\mathbb{R}^8$).\\
  To find out what $v^1$ is, consider $A^T$ which maps movies expressed as a combinations of Up, Skyfall, Thor, Amelie, Snatch, Casablanca, Bridesmaids, Grease to the rating of Aisha, Bob, Chandrika i.e. a vector in $\mathbb{R}^3$. Further more, consider that instead of expressing moves in terms of 8 movies, we want to express a movie in terms on only one fictitious movie which the which most captures the feelings of the raters. This movie corresponds to a certain direction in rater space $\mathbb{R}^3$. That is we are approximating $A^T$ by 
  \beq
  \bar{A^T} = v^1\sigma_1{u^1}^T
  \eeq
%Here, the columns of $U$ form a particularly good basis for the space of ratings of movies. Here the columns (I think) of $V$ form a particularly good basis for representing people. These two things are done keeping $\Sigma$ diagonal.\\
%In other words, the columns of $U$ form a good basis for the space of columns and the columns of $V$ for a good basis for the space of rows. This is useful in low rank approximations.\\
  %In still other words, the columns of $U$ tell you the most important person (which may be some sort of linear combination of Aisha,Bob Chandrika), and the columns of $V$ tell you the most important (or best like) movie (which may be some sort of linear combination of the movie ratings)
\subsection{Code}
\section{SVD Code}
\begin{center}
  \lstinputlisting[language=Python]{learn\_svd\_data.py}
\end{center}   
\section{SVD related math}
Let's see this for a simple $2\times{}2$ case. Let $u^1,u^2$ be the columns of $U$ and $v^1,v^2$ be the columns of $V$. We have
\ber
U =
\begin{pmatrix}
  u^1_1 & u^2_1\\
  u^1_2 & u^2_2
\end{pmatrix}
\qquad
\sigma=
\begin{pmatrix}
  \sigma_1 & 0 \\
  0   &  \sigma_2 
\end{pmatrix}  
\qquad
V =
\begin{pmatrix}
  v^1_1 & v^2_1\\
  v^1_2 & v^2_2
\end{pmatrix} 
\eer
Now, we have
\ber
U\Sigma{V}^T &=& U
\begin{pmatrix}
  \sigma_1 & 0 \\
  0   &  \sigma_2 
\end{pmatrix}
\begin{pmatrix}
  v^1_1 & v^2_1\\
  v^1_2 & v^2_2
\end{pmatrix}^T
=
U
\begin{pmatrix}
  \sigma_1 & 0 \\
  0   &  \sigma_2 
\end{pmatrix}
\begin{pmatrix}
  v^1_1 & v^1_2\\
  v^2_1 & v^2_2
\end{pmatrix}\\
&=& U
\begin{pmatrix}
  \sigma_1v^1_1 & \sigma_1v^1_2\\
  \sigma_2v^2_1 & \sigma_2v^2_2
\end{pmatrix}
= U
\begin{pmatrix}
  \sigma_1{v^1}^T\\
  \sigma_2{v^2}^T
\end{pmatrix}\\
&=&\begin{pmatrix}
u^1 & u^2
\end{pmatrix}
\begin{pmatrix}
  \sigma_1{v^1}^T\\
  \sigma_2{v^2}^T
\end{pmatrix}
= u^1\sigma_1{v^1}^T + u^2\sigma_2{v^2}^T
\eer
%
Now, for SVD of a $2\times{4}$ matrix, we get for $u^{i}$  $\in \mathbb{R}^2$ and $v^{i}$ $\in \mathbb{R}^4$
\ber
%
\begin{pmatrix} u^1 & u^2\end{pmatrix}
\begin{pmatrix} \sigma_1 & 0 & 0 & 0 \\ 0 &\sigma_2 & 0 & 0\end{pmatrix}
\begin{pmatrix}{v^1}^T\\{v^2}^T\\{v^3}^T\\{v^4}^T\end{pmatrix}  
%
&=&
\begin{pmatrix} u^1 & u^2\end{pmatrix}
  \Big{[}
    \begin{pmatrix} \sigma_1 \\ 0 \end{pmatrix}{v^1}^T +
    \begin{pmatrix} 0  \\ \sigma_2 \end{pmatrix}{v^2}^T+
    \begin{pmatrix} 0  \\ 0 \end{pmatrix}{v^3}^T+
    \begin{pmatrix} 0  \\ 0 \end{pmatrix}{v^4}^T 
    \Big{]} \\
  &=&
  \begin{pmatrix} u^1 & u^2\end{pmatrix}
  \begin{pmatrix} \sigma_1{v^1}^T\\ \sigma_2{v^2}^T \end{pmatrix}\\
  &=& \sigma_1u^1{v^1}^T + \sigma_2u^2{v^2}^T
\eer  
%
%
%
\subsection{Another take: columns of $V^T$}
\ber
U =
\begin{pmatrix}
  u^1_1 & u^2_1\\
  u^1_2 & u^2_2
\end{pmatrix}
\qquad
\sigma=
\begin{pmatrix}
  \sigma_1 & 0 \\
  0   &  \sigma_2 
\end{pmatrix}  
\qquad
V^T =
\begin{pmatrix}
  c^1_1 & c^2_1\\
  c^1_2 & c^2_2
\end{pmatrix} 
\eer
Then, we have the svd of $X$ ($x^1,x^2$ are columns)
\ber
\begin{pmatrix}
  x^1 & x^2
\end{pmatrix}
=
U\Sigma{}V^T =
\begin{pmatrix}
u^1 & u^2   
\end{pmatrix}
\begin{pmatrix}
  \sigma_1 c^1_1 & \sigma_1 c^2_1\\
  \sigma_2 c^1_2 & \sigma_2 c^2_2
\end{pmatrix}
=
\begin{pmatrix}
  u^1\sigma_1c^1_1 + u^2\sigma_2c^1_2  & u^1\sigma_1c^2_1 + u^2\sigma_2c^2_2
\end{pmatrix}  
\eer
So, the entries of $c^1$ are determining how $u^1,u^2$ are getting mixed to form  $x^1$. \\
So, the entries of $c^2$ are determining how $u^1,u^2$ are getting mixed to form  $x^2$
%
%
%
\section{Principal component analysis: IISC Notes (Matrix decomposition)}

They say the principal components are
\beq
T = BV =
\begin{pmatrix}
  x_{1}^T\\
  x_{2}^{T}\\
  \vdots\\
  x_{m}^T
\end{pmatrix}
\begin{pmatrix}
  v_1 & v_2 & \hdots & v_n
\end{pmatrix}
=
\begin{pmatrix}
  x_1^Tv_1 & x_1^Tv_2 & \hdots & x_1^Tv_n\\
  x_2^Tv_1 & x_2^Tv_2 & \hdots & x_2^Tv_n\\
  \vdots & \vdots &  \ddots &  \\
  x_m^Tv_1 & x_m^Tv_2 & \hdots & x_m^Tv_n
\end{pmatrix}  
\eeq
%
%
%
\section{IISc Method described above and Ng's method}
In the IISC method, the SVD of
\beq
B = \begin{pmatrix} x_i^T \\ \vdots \\ x_m^T\end{pmatrix}  \qquad \text{ assuming centered data}
\eeq
is computed.
In Ngs method the SVD of 
\beq
\Sigma = \frac{1}{m}\sum_{i=0}^{m}x^{(i)}{x^{(i)}}^{T}
\eeq
is computed. If the SVD of $B$ is $USV^T$ the the SVD of $\Sigma=B^TB$ is $VS^2V^T$. So Ngs $U$ is IISc's $V$.\\
We get
\beq
(V^TX^T) = (XV)^T = (USV^TV)^T = (US)^T  
\eeq
Which is a proof of the important line in the following code.
%
IISc method and Ng's method (\ref{eqn:ngproj}) yield projected matrices that are the transposes of each other. See learn\_svd\_compare\_ng\_iisc.py
\begin{center}
  \lstinputlisting[language=Python]{learn\_svd\_compare\_ng\_iisc.py}
\end{center}  
%
%
%
\section{Why capture (co)variance?}
We want to minimize the difference given in equation (\ref{eqn:minimizedifference}) using a projector of a given rank. Turns out the best projector of a given rank, as measured by equation (\ref{eqn:minimizedifference}), is given by considering the SVD of the co-variance matrix. As more and more rank is retained, we end up capturing more variance.
%
%
%
\section{Why not PCA}
\url{https://blog.kxy.ai/5-reasons-you-should-never-use-pca-for-feature-selection/} See: 5\_Reasons\_You\_Should\_Never\_Use\_PCA\_For\_Feature\_Selection.pdf in
\begin{verbatim}
D:\GoogleDrive\Books\Papers\ML
\end{verbatim}
%
%
%
\section{Kernel PCA}
\url{https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf} or scholkopf\_kernel\_pca.pdf in Books/Papers/ML folder\\
The idea is to compute new coordinates for each data point using equation 10. The LHS of 10 is the projection of the datapoint in high-dimensional space along along the k-th eigenvector. This can be computed using the RHS without computing $\pmb{\phi}(\pmb{x})$ explicitly because all be need, as in SVMs, is the dot-product $(\pmb{\phi}(\pmb{x_i})\cdot\pmb{\phi}(\pmb{x}))$\\

But the thing is, the if you still want the eigenvectors in higher diemnsional space, you still need to explicitly know $\pmb{\phi}$ (equation 5). But the $\alpha_i$ in equation 5 can be solved using a lower dimensional eigenproblem 7. \\

Usually, we don't need to know the higher dimensional eigenvectors. Just using the fact that they're orthogonal, means we can plot a 2D plot or a 3D plot using their coordinates. i.e. one of the orthogonal higher dimensional eigenvectors is along the x axis and the other along the y axis

\section{T-SNE}
T-SNE does not have an inverse transformation to original space. Perhaps this is the reason that it is strictly (ref. IISC homework) for visualization. This is not convincing. We could compute projections using T-SNE and then use the projected data for classification.\\

IISC Lecture: Make historgrams of pair-wise distances in higher dimensional space. Convert this histogram into  pdf. Try to match lower dimensional pdf to higher dimensional pdf by minimizing Kullback-Leibler divergence.

\end{document}

