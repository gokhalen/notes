\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bD}{{\mathbf{D}}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{enumitem}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc}
\usepackage{cancel}
%\usepackage[ngerman]{babel} 
\usepackage{hyperref}
\begin{document}
\title{Notes from Cartoon Guide to Statistics}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Chapter 5: Tale of two distributions}
1. Binomial: mean $np$, variance $np(1-p)$, $p$ is prob of success (1) failure (0)\\
2. Normal: pdf
\beq
f(x|\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{\mu-\sigma}{\sigma})^2}
\eeq
$\mu$ is mean and $\sigma$ is standard deviation.\\
3. Binomial distribution is assymetric but assymetry gets overwhelmed when $n$ is large.\\
4. Fuzzy CLT: data that are influenced by many small and unrelated random effects are approximately normally distributed.
\beq
\text{Pr}(a\le{x}\le{b}) = F(\frac{b-\mu}{\sigma}) - F(\frac{a-\mu}{\sigma})
\eeq
5. Normal as a good approximation to binomial $np \ge 5 $ and $n(1-p)\ge 5$
\section{CGS Chapter 6: Sampling}
1. Sampling for defective pieces: sample size $n$, $x$ number of good pieces, $\hat{p}=\frac{x}{n}$. How is $\hat{p}$ distributed around $p$ (the true success rate). $\hat{p}$ is an \textit{estimator} of $p$. Binomial process, $x$ successes in $n$ trials.\\
2. Let $\hat{P}$ be the random variable and let $\hat{p}$ be the values it takes.\\
3. We know (by magic, an because a binomial distribution can be approximated by normal)
\ber
E(\hat{P}) = p , \sigma(\hat{P}) = \sqrt{\frac{p(1-p)}{n}}\\
\hat{P}\sim\mathcal{N}(p,\sigma(\hat{P})), \text{ the normal distribution}
\eer
4. \textit{Estimate} is a single measure or observation. \textit{Estimator} is a rule for getting estimates.\\
\subsection{Sampling distribution of the mean}
1. Need to know average length of pickles. \\
2. Randomly select $n$ pickles and measure their lengths $x_1,x_2,\cdots,x_n$.\\
3. Let $X_i$ be a random variable allowed to take value $x_i$, the length of the $i^{th}$ pickle.\\\\
By pretty much the definition of $\mu$ and $\sigma$, we have
\beq
E(X_i) = \mu \text{ the true mean }, \sigma(X_i) = \sigma \text{ the true standard deviation} \\
\eeq
\ber
\bar{X} &=& \frac{X_1+X_2+\cdots+X_n}{n}\\
E(\bar{X}) &=& E\Big(\frac{X_1+X_2+\cdots+X_n}{n}\Big)\\
  &=& \frac{1}{n}(E(X_1) + E(X_2) + \cdots + E(X_n))\\
&=& \frac{1}{n}(\mu+\mu+\cdots+\mu) = \mu\\
\sigma^2(\bar{X}) &=&  \sigma^2\Big(\frac{X_1+X_2+\cdots+X_n}{n}\Big)\\
&=& \frac{1}{n^2}\sigma^2(X_1+X_2+\cdots+X_n) \\
&=&  \frac{1}{n^2}\Big(\sigma^2(X_1)+\sigma^2(X_2)+\cdots+\sigma^2(X_n)\Big)\\
&=& \frac{1}{n^2}(\sigma^2+\sigma^2+\cdots+\sigma^2)\\
&=& \frac{\sigma^2}{n}\\
E(\bar{X}) &=& \mu \qquad \sigma(\bar{X}) = \frac{\sigma}{\sqrt{n}}\\
\bar{X} &\sim& \mathcal{N}(\mu,\frac{\sigma}{\sqrt{n}})
\eer
\subsection{t-distribution}
1. Need because of problems with CLT. We need large $n$ and we have no knowledge of $\sigma$.\\
2  Solution: estimate $\sigma$ using
\beq
s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2}
\eeq
3. In place of the random-variable
\beq
Z = \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}
\eeq
use
\beq
t=\frac{\bar{X}-\mu}{\frac{s}{\sqrt{n}}}
\eeq
$t$ is distributed according to the student's-t distribution and approaches normal distribution as $n\rightarrow\infty$.
\section{CGS Chapter 7: Confidence intervals}
\subsection{For true or false type problems}
1. For true or false type problems\\
2. $p$ fraction of candidates who vote for Mr. Astute. We don't know $p$ and are trying to estimate it.\\
3. We take a sample of 1000 voters and determine $\hat{p}=\frac{550}{1000}$. $\hat{p}$ is an estimator of $p$. This is a binomial process and can be approximated by a normal distribution.
4. We want to get limits  of the form
\beq
p \in \hat{p} \pm \text{error} , \text{ say 95\% of the time}
\eeq
5. We know (by magic)
\beq
E(\hat{p}) = p \text{ and } \sigma(\hat{p}) = \sqrt{\frac{p(1-p)}{n}} \text{ and } \hat{p} \sim \mathcal{N}(p,\sigma(\hat{p}))
\eeq
Because $\hat{p} \sim \mathcal{N}(p,\sigma(\hat{p}))$, if we repeat the sampling experiment a large number of times, 95\% of the time we are going to get 
\beq
Z = \frac{\hat{p}-p}{\sigma(\hat{p})}
\eeq
within
\beq
-1.96 \le Z \le 1.96,
\eeq
where the 1.96 is linked to the 95\% confidence using standard normal pdf. Therefore,
\ber
-1.96 \le \frac{\hat{p}-p}{\sigma(\hat{p})} \le 1.96 \\
-1.96\sigma(\hat{p}) \le \hat{p}-p \le 1.96\sigma(\hat{p}) \\
-1.96\sigma(\hat{p}) - \hat{p}\le -p \le 1.96\sigma(\hat{p}) - \hat{p}
\eer
Since multiplying by -1 reverses inequalities,
\ber
1.96\sigma(\hat{p}) + \hat{p} \ge p \ge -1.96\sigma(\hat{p}) + \hat{p} 
\eer
Rearranging
\beq
\hat{p} -1.96\sigma(\hat{p}) \le p \le \hat{p} + 1.96\sigma(\hat{p})
\eeq
So we can say,
\beq
p \in \hat{p} \pm 1.96\sigma(\hat{p}), \text{ with 95\% confidence }
\eeq
In practice, we will approximate $\sigma(\hat{p})$ using
\beq
\text{SE}(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\eeq
So, finally, we can say
\beq
p \in \hat{p} \pm 1.96\text{SE}(\hat{p}), \text{ with 95\% confidence }
\eeq
\subsubsection{Increasing confidence}
The bounds derived previously are of the form
\beq
p \in \hat{p} \pm \text{E}
\eeq
where $E$ is the error given by
\beq
E = Z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}, \,\, 1-\alpha \text{ is the confidence }
\eeq
There are two things we can do:
1. Given $Z_{\alpha/2},n,\hat{p}$, we can find $E$.\\
2. Given $E,Z_{\alpha/2},\hat{p}$, we can find the sample size $n$.
Typically large $n$ is not useful in practice because we run into problems with bias. Therefore \$ are better spent reducing bias.
\subsection{Confidence intervals for the mean}
\beq
\bar{X} \sim \mathcal{N}(\mu,\frac{\sigma}{\sqrt{n}})
\eeq
As before because $\bar{X}\sim\mathcal{N}$, we can see if we compute the mean by sampling a large number of times, 95\% of the time, we will get
\beq
-1.96 \le Z=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \le 1.96
\eeq
Replace $\sigma$ by $s$ the sample standard deviation. We get finally,
\beq
\bar{X} - 1.96\text{SE}(\bar{X}) \le \mu \le \bar{X} + 1.96\text{SE}(\bar{X}),
\eeq
where $\text{SE}(\bar{X})=\frac{s}{\sqrt{n}}$.
\subsection{Confidence intervals for students-t}
1. For small sample sizes.
2. Choose distribution with $n-1$ degrees of freedom.
3. For confidence level $(1-\alpha)$ we can say,
\beq
\bar{X} - t_{\alpha/2}\text{SE}(\bar{X}) \le \mu \le \bar{X} + t_{\alpha/2}\text{SE}(\bar{X}),
\eeq
where,
\beq
SE(\bar{X}) = \frac{s}{\sqrt{n}}
\eeq
\section{CGS: Chapter 8, Hypothesis testing}
\subsection{Basic idea}
1. It's all about answering the question ``Could these observations really have occurred by chance?'' \\
2. Steps:
\begin{enumerate}
\item{Formulate hypothesis: $H_0$: Null Hypothesis, $H_a$: Alternate hypothesis}
\item{$H_o$: usually that the obervations ocurred purely by chance.}
\item{Identify test-statistic: $Z$ and its distribution as implied by $H_0$.}
\item{Choose p-value: If $H_0$ is true, what is the chance of observing something as extreme as $Z$. Basically Pr$(Z)$.}
\item{Level of significance $\alpha$: if p-value $<$ alpha, reject $H_0$}  
\end{enumerate}
\subsection{Large sample significance test for proportions}
1. By proportions we mean random variable can take only true or false values.
\begin{enumerate}
\item{{\textbf{Step 1}} Null Hypothesis: $H_0: p = p_0$ Note: we choose $p_0$}
  \begin{enumerate}[label=(\alph*)]
  \item{$H_a: p \ge p_0$}
  \item{$H_a: p \le p_0$}
  \item{$H_a: p \ne p_0$}  
  \end{enumerate}
\item{{\textbf{Step 2}} Test statistic:  \beq Z_{obs} = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} \eeq}
\item{{\textbf{Step 3}} Compute p-values acccording to which alternate hypothesis $H_a$ was chosen}
  \begin{enumerate}[label=(\alph*)]
  \item{\beq\text{p-value}=\text{Pr}(z>Z_{obs})\eeq}
  \item{\beq\text{p-value}=\text{Pr}(z<Z_{obs})\eeq}
  \item{\beq\text{p-value}=\text{Pr}(|z|<|Z_{obs}|)\eeq}
  \end{enumerate}
\item{{\textbf{Step 4}} If p-value $<\,\alpha$ reject $H_0$ and accept $H_a$.}
\end{enumerate}
\subsection{Large sample test for the mean}
Same as before, except:
\beq
Z_{obs} = \frac{\bar{X}-\mu_0}{\frac{s}{\sqrt{n}}}
\eeq
The p-values are as before. Can combine $\mu_{low} \le \mu \le \mu_{high}$ by doing two tests, one for $\mu_{low} \le \mu$ and one for $\mu \le \mu_{high}$.
\subsection{Small sample test for the mean}
Same as before, except
\beq
t_{obs} = \frac{\bar{X}-\mu_0}{\text{SE}(\bar{X})}
\eeq
And choose t-distribution with $n-1$ dof.
\subsection{Decision theory}
$H_0:$ no fire, $H_a$: fire.
\beq
\beta = \text{Pr}(Accept H_0 | H_a)
\eeq
$1-\beta$ is the power of the test.
\section{Chapter 9: Comparing two populations}
\subsection{Binary variable}
1. Binary variable, true or false
2. Population 1: success $p_1$, Population 2: success $p_2$
3. Need to estimate $p_1-p_2$.
Our statistic will be
\beq
Z = \frac{\hat{p}_1-\hat{p}_2 - (p_1-p_2)}{\sigma(\hat{p}_1-\hat{p}_2)}, \quad Z \sim \mathcal{N}(0,1)
\eeq
\ber
\sigma^2(\hat{p}_1-\hat{p}_2) &=& \sigma^2(\hat{p}_1) + \sigma^2(\hat{p}_2) \quad \text{ independence } \\
\sigma(\hat{p}_1-\hat{p}_2) &=& \sqrt{\sigma^2(\hat{p}_1) + \sigma^2(\hat{p}_2)}
\eer
Approximate $\sigma(\hat{p}_1-\hat{p}_2)$ by
\beq
\text{SE}(\hat{p}_1-\hat{p}_2) = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
\eeq
As before we can compute a confidence interval for given ($1-\alpha$):
\beq
p_1 - p_2 \in \hat{p}_1-\hat{p}_2 \pm z_{\alpha/2}\text{SE}(\hat{p}_1-\hat{p}_2) 
\eeq
\subsection{Hypothesis testing}
$H_0:$ Aspirin had no effect $p_1=p_2$.\\
$H_a:$ Aspirin reduces heart attack rate $p_1>p_2$\\
Need a statistic
\ber
Z &=& \frac{\hat{p}_1-\hat{p}_2 - \cancelto{0, H_0 \implies p_1-p_2=0}{(p_1-p_2)}}{\sigma(\hat{p}_1-\hat{p}_2)} \\
Z &=& \frac{\hat{p}_1-\hat{p}_2}{\sigma(\hat{p}_1-\hat{p}_2)} 
\eer
Approximate as before
\beq
\sigma(\hat{p}_1-\hat{p}_2) \approx  \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
\eeq
Since $H_0 \implies p_1-p_2=0$, replace $p_1$ and $p_2$ by $\hat{p}$ where
\beq
\hat{p} = \frac{x_1+x_2}{n_1+n_2}
\eeq
So, we have,
\beq
\sigma(\hat{p}_1-\hat{p}_2) \approx \sqrt{\hat{p}(1-\hat{p})\Big(\frac{1}{n_1} + \frac{1}{n_2}\Big)} = \text{SE}_0(\hat{p}_1-\hat{p}_2)    
\eeq
Our statistic now is
\beq
Z = \frac{\hat{p}_1 - \hat{p}_2}{\text{SE}_0(\hat{p}_1-\hat{p}_2)} \, \sim \mathcal{N}(0,1)
\eeq
\subsection{General recipie}
Null hypothesis $H_0: p_1 = p_2$\\
Our statistic is
\beq
Z_{obs} = \frac{\hat{p}_1-\hat{p}_2}{\text{SE}_0(\hat{p}_1-\hat{p}_2)}
\eeq
Three alternate hypothesis:\\
a) $H_a:p_1\ne{}p_2$ p-value = Pr$ (|z|>|Z_{obs}|)$ \\
b) $H_a:p_1>{}p_2$ p-value = Pr$ (z>Z_{obs})$\\
c) $H_a:p_1<{}p_2$ p-value = Pr$ (z<Z_{obs})$
Reject Null Hypothesis if
\beq
\text{p-value} < \alpha
\eeq
\subsection{Comparing means of two populations}
Population 1: $\mu_1,\sigma_1$\\
Population 2: $\mu_2,\sigma_2$\\
Statistic:
\beq
Z = \frac{\bar{X}_1-\bar{X}_2 - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma^2(\mu_1)}{n_1}+\frac{\sigma^2(\mu_2)}{n_2}}} \sim \mathcal{N}(0,1) \\
\eeq
We approximate
\beq
\sqrt{\frac{\sigma^2(\mu_1)}{n_1}+\frac{\sigma^2(\mu_2)}{n_2}} \approx \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}} = \text{SE}(\bar{X}_1-\bar{X}_2)
\eeq
As before, for $1-\alpha$ confidence intervals
\beq
\mu_1 - \mu_2 \in \hat{x}_1 - \hat{x}_2 \pm z_{\alpha/2}\text{SE}(\bar{X}_1-\bar{X}_2)
\eeq
\subsection{Hypothesis testing for means of two populations}
$H_0:\mu_1 = \mu_2$ (usually the hypothesis that things occur by chance)
\beq
Z_{obs} = \frac{\hat{X}_1-\hat{X}_2-\cancelto{H_0 \implies \mu_1-\mu_2=0}{(\mu_1-\mu_2)}}{\text{SE}(\hat{X}_1-\hat{X}_2)}
\eeq
a) $H_a: \mu_1 > \mu_2$: p-value=Pr$(z>Z_{obs})$\\
b) $H_a: \mu_1 < \mu_2$: p-value=Pr$(z<z_{obs})$\\
c) $H_a:\mu_1\ne\mu_2$: p-value=Pr$(|z|>|Z_{obs})$\\
If p-value < $\alpha$, reject Null Hypothesis
\subsection{Small sample means}
Write
\beq
s^2_{pool} = \frac{ (n_1-1)s^2_1 + (n_2-1)s^2_2}{n_1 + n_2 - 2}
\eeq
Redefine $\text{SE}$ in the previous section as
\beq
\text{SE}(\hat{X}_1- \hat{X}_2) = \sqrt{\frac{s^2_{pool}}{n_1} + \frac{s^2_{pool}}{n_2}} = s_{pool}\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
\eeq
The $1-\alpha$ confidence interval is
\beq
\mu_1 - \mu_2 = \hat{x}_1 - \hat{x}_2 \pm t_{\alpha/2}SE(\bar{X}_1-\bar{X}_2)
\eeq
\subsection{Hypothesis testing for t-distributions}
We make a single population out of the two populations, by the transformation
$d=x_1-x_2$. Under the null-hypothesis $H_0:x_1-x_2=0, i.e. d = 0$, the statistic
\beq
t_{obs} = \frac{\hat{d}-\cancelto{0}{d}}{\frac{s_d}{\sqrt{n}}} \text{ is t-distributed}
\eeq
95\% confidence level
\beq
\mu_d \in \bar{d} \pm t_{0.025}(\frac{s_d}{\sqrt{n}})
\eeq
$t_{0.025}$ is potentially from $n-1$ dof t-distribution.\\
As before $H_o: d=0$\\
1. $H_a:d>0$ p-value = Pr$(t>t_{obs})$\\
2. $H_a:d<0$ p-value = Pr$(t<t_{obs})$\\
3. $H_a:d\ne{0}$ p-value = Pr$(|t|>|t_{obs}|)$\\
p-value $<\alpha$ reject null hypothesis.
\section{Chap 11 Regression}
Given data $(x_i,y_i)$. Minimize
\beq
E(a,b) = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 \text{ where } \hat{y}_i = a +bx_i
\eeq
$\pdd{E}{a}=0$ gives
\ber
\sum_{i=1}^{i=n}(y_i-a-bx_i)(-1) = 0 \\
n\bar{y} - an -bn\bar{x} = 0 \text{ where } \bar{y} = \frac{1}{n}\sum_{i=0}^{i=n}y_i, \bar{x} = \frac{1}{n}\sum_{i=0}^{i=n}x_i \\
\bar{y} = a + b\bar{x}  
\eer
$\pdd{E}{b}=0$ yields
\ber
\sum_{i=1}^{n}(y_i - a - bx_i)x_i = 0 \\
\sum_{i=1}^{n}(y_ix_i - ax_i -bx_i^2) = 0\\
\sum_{i=1}^{n}y_ix_i - an\bar{x} - b\sum_{i=1}^n x_i^2 = 0\\
\sum_{i=1}^{n}y_ix_i - ( \bar{y} - b\bar{x})n\bar{x} - b\sum_{i=1}^n x_i^2=0\\
b = \frac{\sum_{i=1}^{n}y_ix_i-n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2- n\bar{x}^2}
\eer
The denominaror from CGS is
\ber
\text{deno} &=& \sum( x_i^2 - 2x_i\bar{x} + \bar{x}^2) \\
&=& \sum ( x_i^2 - 2\bar{x}n\bar{x} + n\bar{x}^2) \\
&=& \sum ( x_i^2 - n\bar{x}^2)
\eer
which is our denominator.\\
The numerator from CGS is
\ber
\text{ numer } &=& \sum(x_i-\bar{x})(y_i-\bar{y})\\
&=& \sum(x_iy_i - \bar{x}y_i -\bar{y}x_i + \bar{x}\bar{y})\\
=\sum(x_iy_i) - \bar{x}n\bar{y}  - \bar{y}n\bar{x} + n\bar{x}\bar{y} \\
=\sum(x_iy_i) - n\bar{x}\bar{y}
\eer
which is our numerator.
\subsection{Orthogonality}
\beq
y = \begin{bmatrix}y_1 \\ \vdots \\ y_n \end{bmatrix}, X = \begin{bmatrix}1 & x_1\\  \vdots & \vdots \\ 1 & x_n \end{bmatrix}, \beta =  \begin{bmatrix}a \\ b \end{bmatrix}
\eeq
Then, in regression, we minimize
\beq
\pi = \|y-X\beta\|_2^2
\eeq
wrt $\beta$ and we get
\beq
\beta = (X^TX)^{-1}X^Ty
\eeq
Our predictions are
\beq
\hat{y} = X\beta
\eeq
The residual is $y-\hat{y}$, which is orthogona to the rows in $X^T$.
\ber
\hat{e} &=& y - \hat{y}\\
X^T\hat{e} &=& X^T(y-\hat{y})\\
&=& X^Ty - X^TX\beta\\
&=& X^Ty - X^TX(X^TX)^{-1}X^Ty \\
&=& X^Ty - X^Ty = 0
\eer
The error between the mean $\bar{y}$ and prediction is
\ber
\|y-\bar{y}\|_2^2 &=& \|y-\hat{y} + \hat{y}-\bar{y}\|^2_2 \\
&=& \|y-\hat{y}\|^2_2 + \|\hat{y}-\bar{y}\|^2_2 + 2(\hat{y}-\bar{y})^T(y-\hat{y})
\eer
We shall now show that the last term is $0$. The last term is 
\beq
(\hat{y}-\bar{y})^T(y-\hat{y}) = (\hat{y}-\bar{y})^Te = \hat{y}^Te-\bar{y}^Te
\eeq
Both $\hat{y}^Te,\bar{y}^Te$ are identically zero. The first term is easy and we shall start with it first.
\beq
\hat{y}^Te = (X\beta)^Te = \beta^T\cancelto{0}{X^Te} = 0
\eeq
The $\bar{y}$ in the second term can be written as
\ber
\bar{y} &=& \begin{bmatrix}a + b\bar{x}\\ \vdots \\ a + b\bar{x} \end{bmatrix}\\
&=& \begin{bmatrix}a & b\bar{x}\\ \vdots & \vdots \\ a & b\bar{x} \end{bmatrix}\begin{bmatrix}1 \\ 1\end{bmatrix}\\
&=& \begin{bmatrix}1 & \bar{x}\\ \vdots & \vdots \\ 1 & \bar{x} \end{bmatrix}\begin{bmatrix}a \\ b\end{bmatrix}\\
&=& Y\beta    
%&=& \frac{1}{n}\begin{bmatrix}
%  \begin{bmatrix}1 & x_1 \\ \vdots & \vdots \\ 1 & x_1\end{bmatrix} & + &
%    \begin{bmatrix}1 & x_2 \\ \vdots & \vdots \\ 1 & x_2\end{bmatrix} &  + &
%      \hdots & + &
%  \begin{bmatrix}1 & x_n \\ \vdots & \vdots \\ 1 & x_n\end{bmatrix}
%\end{bmatrix}
%\begin{bmatrix} a \\ b\end{bmatrix}
\eer
Now comes the interesting part. Both columns of $Y$ are proportional to
\beq
\begin{bmatrix}1\\\vdots\\1\end{bmatrix}
\eeq
We know one column of $X$ is
\beq
\begin{bmatrix}1\\\vdots\\1\end{bmatrix}
\eeq
Since $X^Te=0$, we must have
\beq
\begin{bmatrix}1 \hdots 1 \end{bmatrix}e = 0
\eeq
because $\begin{bmatrix}1 \hdots 1 \end{bmatrix}$ is the first row of $X^T$. Similarly
\beq
Y^Te = 0
\eeq
because all rows of $Y^T$ are proportional to
\beq
\begin{bmatrix}1\hdots1\end{bmatrix}
\eeq.
Now we are ready to use $Y^Te=0$.
\ber
\bar{y}^Te = (Y\beta)^Te = \beta^T\cancelto{0}{Y^Te} = 0 
\eer
So we have
\beq
(\hat{y}-\bar{y})^T(y-\hat{y}) = (\hat{y}-\bar{y})^Te = \hat{y}^Te -  \bar{y}^Te = 0
\eeq
So, finally we have
\beq
\|y-\bar{y}\|_2^2 = \overbrace{\|y-\hat{y}\|^2_2}^\text{bias term/data fidelity} + \overbrace{\|\hat{y}-\bar{y}\|^2_2}^\text{variance} 
\eeq
Define
\ber
\text{SSR} = \|\hat{y}-\bar{y}\|^2_2 \qquad \text{SSE} = \|y-\hat{y}\|^2_2 \qquad \text{SS}_{yy}=\|y-\bar{y}\|_2^2
\eer
The correlation coefficient is
\beq
r = \text{sign}(b)\sqrt{R^2} \qquad \text{where } R^2 = 1 - \frac{\text{SSE}}{\text{SS}_{yy}}
\eeq
$R^2$ of the variation in $y$ is explained by $x$.
\subsection{Statistical viewpoint}
A regression model
\beq
Y = \alpha + \beta{x} + \epsilon
\eeq
$\alpha$, $\beta$ are true unknown parameters we seek to estimate. $\epsilon$ is random error fluctuations. $a$ and $b$ are best linear unbiased estimators of $\alpha$, $\beta$. i.e. $E(a)=\alpha, E(b)=\beta$. Different samples $(y_i,x_i)$, yield different $a,b$. How are they distributed about $\alpha_{true},\beta_{true}$?\\
For each data point $y_i$ we have
\beq
y_i = a + bx_i + e_i\\
\eeq
An estimator for $\sigma(e)$ is
\beq
s = \sqrt{\frac{\sum_{i=1}^{n}e_i^2}{n-2}} = \sqrt{\frac{\text{SS}_{yy}-b\text{SS}_{xy}}{n-2}}
\eeq
$s$ is an estimator for how widely the points will be scattered around the line.
\subsection{Confidence intervals}
The $1-\alpha$ confidence intervals have the familiar form.
\ber
\beta  &=& b \pm t_{\alpha/2}\text{SE}(b)\\
\alpha &=& a \pm t_{\alpha/2}\text{SE}(a)
\eer
where
\ber
\text{SE}(b) = \frac{s}{\sqrt{SS_{xx}}}\\
\text{SE}(a) = s\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\text{SS}_{xx}}}
\eer
If we repeat the experiment many times, $\alpha,\beta$ will lie in the given interval $1-alpha$\% times
\subsection{Mean response}
Given $x_0$ what is the mean response $y$? The 95\% confidence interval for
\beq
y = \alpha + \beta{}x_0
\eeq
is
\beq
y \in a + bx_0 \pm t_{0.25}\text{SE}(\hat{y})
\eeq
where
\beq
\text{SE}(\hat{y}) = s\sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\text{SS}_{xx}}}
\eeq
\subsection{Hypothesis testing}
Is there relationship between height and weight?\\
$H_0:\beta=0$: Null hypothesis $\beta=0$ (no relation between height and weight)\\
Statistic:
\beq
t = \frac{b-\cancelto{0}{\beta}}{\text{SE}(b)} = \frac{b}{\text{SE}(b)}
\eeq
Alternate hypothesis
Need $t>t_{alpha}$ for $H_a: \beta>0$\\
Need $t<t_{alpha}$ for $H_a: \beta<0$\\
Need $|t|<|t_{alpha/2}|$ for $H_a: \beta\ne{0}$\\
\subsection{Patterns in error}
Pattern in error Vs $\hat{y}$ indicates problems. Random scatterplot is ok. 
\end{document}


