\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\begin{document}
\title{Support vector machines}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Setup of the optimization problem}
We want to find an hyperplane which separates the points and there is maximal separation (distance) between the points and the hyperplane. This is based on Prof Deepak Subramani's notes and Element's of statistical learning section by Hastie 4.5.2.\\

The distance of a point $\pmb{x}^{(j)}$ from a plane defined by $\pmb{\beta}^{T}\pmb{x} + \beta_0 = 0$ is given by:

\beq
d(\pmb{x^{(j)}}) = \frac{\pmb{\beta}^T\pmb{x^{(j)}} + \beta_0}{\|\pmb{\beta}\|}
\eeq

This distance has a sign. It will be positive on one side of the hyperplane and negative on the other side, depending on the signs of $\pmb{\beta},\beta_0$. We can make it always positive by multiplying it by a function $t^{(j)}$ which is $+1$ or $-1$ depending on which side of the plane the training point $\pmb{x^{(j)}}$ lies. So our modified distance function becomes

\beq
d(\pmb{x^{(j)}}) = \frac{t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)}{\|\pmb{\beta}\|}
\eeq

Note: there is no implied summation over $j$. Then, our optimization problem becomes

\beq
\label{eqn:formulation1}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0,M} &  \qquad M \\
  \textrm{s.t.} & \qquad \frac{t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)}{\|\pmb{\beta}\|} \ge\, M, \, \qquad \forall \,{j}
\end{aligned}
\eeq

We make the following substitution

\beq
\label{eqn:subs1}
M = \frac{M'}{\|\pmb{\beta}\|}
\eeq

Note that $\|\pmb{\beta}\|\ne{0}$ because if $\pmb{\beta}=0$ there is no hyperplane because the hyperplane equation reduces to $\beta_0=0$ because  $\pmb{\beta}=0 \implies {\beta_i}=0, i\ne{0}$. This condition, $\|\pmb{\beta}\|\ne{0}$, will have to be enforced in the optimization program.\\

Using equation ($\ref{eqn:subs1}$) in ($\ref{eqn:formulation1}$) we get:

\beq
\label{eqn:formulation2}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0,M'} &  \qquad \frac{M'}{\|\pmb{\beta}\|} \\
  \textrm{s.t.} & \qquad \frac{t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)}{\|\pmb{\beta}\|} \ge\, \frac{M'}{\|\pmb{\beta}\|}  \, \qquad \forall \,{j}
\end{aligned}
\eeq

The $\|\pmb{\beta}\|$ in the denominator is +ve and it cancels. The problem becomes

\beq
\label{eqn:formulation3}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0,M'} &  \qquad \frac{M'}{\|\pmb{\beta}\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)} \ge\, {M'}  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Change variables again. This time make the substitution

\beq
\label{eqn:subs2}
\pmb{\beta} = M'\pmb{\beta}' \implies \|\pmb{\beta}\| = M'\|\pmb{\beta}'\|    \qquad \beta_0 = M'\beta'_0. 
\eeq

Using ($\ref{eqn:subs2}$) in ($\ref{eqn:formulation3}$), we get the following optimization problem

\beq
\label{eqn:formulation4}
\begin{aligned}
  \max_{\pmb{\beta}',\beta'_0,M'} &  \qquad \frac{M'}{M'\|\pmb{\beta}'\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}M'({\pmb{\beta}'}^T{\pmb{x^{(j)}}} + \beta'_0)} \ge\, {M'}  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Cancelling $M'$ everywhere, we see it drops out of the optimization problem, resulting in

\beq
\label{eqn:formulation5}
\begin{aligned}
  \max_{\pmb{\beta}',\beta'_0} &  \qquad \frac{1}{\|\pmb{\beta}'\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}({\pmb{\beta}'}^T{\pmb{x^{(j)}}} + \beta'_0)} \ge\, 1  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Drop the superscript $'$ to get 

\beq
\label{eqn:formulation6}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0} &  \qquad \frac{1}{\|\pmb{\beta}\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}({\pmb{\beta}}^T{\pmb{x^{(j)}}} + \beta_0)} \ge \, 1  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Convert the maximization problem to a minimization problem by taking the reciprocal of the objective function. Square it and add $(1/2)$ for prettiness. This is our SVM optimization problem

\beq
\label{eqn:formulation7}
\begin{aligned}
  \min_{\pmb{\beta},\beta_0} &  \qquad \frac{1}{2}{\|\pmb{\beta}\|^2} \\
  \textrm{s.t.} & \qquad {t^{(j)}({\pmb{\beta}}^T{\pmb{x^{(j)}}} + \beta_0)} \ge \, 1  \, \qquad \forall \,{j}
\end{aligned}
\eeq

These transformations are combined into one step in the book Elements of Statistical Learning.

\section{Setting minimum distance}

We add a constraint $t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)=1$ for the closest point - we are NOT assuming the knowledge of the point closest to the plane. We are just saying that there is a closest point and we will find that closest point as part of the optimization program.

\section{Soft margin classifier}
Instead of explicitly enforcing constraints, penelize them.

\section{SVM vs logistic regression with poly features}
- Logistic regression is hard to train with polyfeatures\\
- Kernel trick in SVM means you don't need to explicitly compute features\\
- Kernel trick comes from the dual of the quadratic optimization problem.\\
- Before deeplearning designing kernels used to be a research effort\\
- LR is probability based, SVM is not\\
- SVM does not support calculating probs directly\\

\section{Gaussian Radial basis features}
- Create new feature for every data point
- In theory it introduces infinite features (one for every data point). this is in contrast to creating polynomial features, where the number of features is not dependent on the number of data points.

\section{RKHS theorem}
- any non-separable theorem can be separated in an appropriate higher dimensional space.

\section{Links}
\url{https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe}

\section{Support vector regression}
``Support Vector Regression is a supervised learning algorithm that is used to predict discrete values. Support Vector Regression uses the same principle as the SVMs. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points (within a tolerance - nhg).'' \url{https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0} But what is the dependent variable? Suppose we are given data points $(x_i,y_i)$. We use a support vector regression to determine the line which contains most of the points. This line is $y=wx+b$. When we see a new point $x_{new}$, put it in and get $y_{new}$. i.e. $y_{new} = wx_{new}+b$

\section{Complexity of the primal and dual problem}
- Check profs notes


\end{document}
