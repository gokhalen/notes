\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\ddeps}{\frac{d}{d\epsilon}\Big{|}_{\rightarrow{0}}}
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\begin{document}
\title{Support vector machines}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Why is maximum margin desired}
\url{https://www.quora.com/Why-would-we-prefer-a-large-margin-running-a-SVM}
``A large margin effectively corresponds to a regularization of SVM weights which prevents overfitting. Hence, we prefer a large margin (or the right margin chosen by cross-validation) because it helps us generalize our predictions and perform better on the test data by not overfitting the model to the training data.''


\section{Setup of the optimization problem}
We want to find an hyperplane which separates the points and there is maximal separation (distance) between the points and the hyperplane. This is based on Prof Deepak Subramani's notes and Element's of statistical learning section by Hastie 4.5.2.\\

The distance of a point $\pmb{x}^{(j)}$ from a plane defined by $\pmb{\beta}^{T}\pmb{x} + \beta_0 = 0$ is given by:

\beq
d(\pmb{x^{(j)}}) = \frac{\pmb{\beta}^T\pmb{x^{(j)}} + \beta_0}{\|\pmb{\beta}\|}
\eeq

This distance has a sign. It will be positive on one side of the hyperplane and negative on the other side, depending on the signs of $\pmb{\beta},\beta_0$. We can make it always positive by multiplying it by a function $t^{(j)}$ which is $+1$ or $-1$ depending on which side of the plane the training point $\pmb{x^{(j)}}$ lies. So our modified distance function becomes

\beq
d(\pmb{x^{(j)}}) = \frac{t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)}{\|\pmb{\beta}\|}
\eeq

Note: there is no implied summation over $j$. Then, our optimization problem becomes

\beq
\label{eqn:formulation1}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0,M} & \qquad M \\
  \textrm{s.t.} & \qquad \frac{t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)}{\|\pmb{\beta}\|} \ge\, M, \, \qquad \forall \,{j}
\end{aligned}
\eeq

We make the following substitution

\beq
\label{eqn:subs1}
M = \frac{M'}{\|\pmb{\beta}\|}  \text{ where } M' > 0
\eeq 

Note that $\|\pmb{\beta}\|\ne{0}$ because if $\pmb{\beta}=0$ there is no hyperplane because the hyperplane equation reduces to $\beta_0=0$ because  $\pmb{\beta}=0 \implies {\beta_i}=0, i\ne{0}$. This condition, $\|\pmb{\beta}\|\ne{0}$, will have to be enforced in the optimization program.\\

Using equation ($\ref{eqn:subs1}$) in ($\ref{eqn:formulation1}$) we get:

\beq
\label{eqn:formulation2}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0,M'} &  \qquad \frac{M'}{\|\pmb{\beta}\|} \\
  \textrm{s.t.} & \qquad \frac{t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)}{\|\pmb{\beta}\|} \ge\, \frac{M'}{\|\pmb{\beta}\|}  \, \qquad \forall \,{j}
\end{aligned}
\eeq

The $\|\pmb{\beta}\|$ in the denominator is +ve and it cancels. The problem becomes

\beq
\label{eqn:formulation3}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0,M'} &  \qquad \frac{M'}{\|\pmb{\beta}\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}(\pmb{\beta}^T{\pmb{x^{(j)}}} + \beta_0)} \ge\, {M'}  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Change variables again. This time make the substitution

\beq
\label{eqn:subs2}
\pmb{\beta} = M'\pmb{\beta}' \implies \|\pmb{\beta}\| = M'\|\pmb{\beta}'\|    \qquad \beta_0 = M'\beta'_0. 
\eeq

Using ($\ref{eqn:subs2}$) in ($\ref{eqn:formulation3}$), we get the following optimization problem

\beq
\label{eqn:formulation4}
\begin{aligned}
  \max_{\pmb{\beta}',\beta'_0,M'} &  \qquad \frac{M'}{M'\|\pmb{\beta}'\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}M'({\pmb{\beta}'}^T{\pmb{x^{(j)}}} + \beta'_0)} \ge\, {M'}  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Cancelling $M'$ everywhere, we see it drops out of the optimization problem, resulting in

\beq
\label{eqn:formulation5}
\begin{aligned}
  \max_{\pmb{\beta}',\beta'_0} &  \qquad \frac{1}{\|\pmb{\beta}'\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}({\pmb{\beta}'}^T{\pmb{x^{(j)}}} + \beta'_0)} \ge\, 1  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Drop the superscript $'$ to get 

\beq
\label{eqn:formulation6}
\begin{aligned}
  \max_{\pmb{\beta},\beta_0} &  \qquad \frac{1}{\|\pmb{\beta}\|} \\
  \textrm{s.t.} & \qquad {t^{(j)}({\pmb{\beta}}^T{\pmb{x^{(j)}}} + \beta_0)} \ge \, 1  \, \qquad \forall \,{j}
\end{aligned}
\eeq

Convert the maximization problem to a minimization problem by taking the reciprocal of the objective function. Square it and add $(1/2)$ for prettiness. This is our SVM optimization problem

\beq
\label{eqn:formulation7}
\begin{aligned}
  \min_{\pmb{\beta},\beta_0} &  \qquad \frac{1}{2}{\|\pmb{\beta}\|^2} \\
  \textrm{s.t.} & \qquad {t^{(j)}({\pmb{\beta}}^T{\pmb{x^{(j)}}} + \beta_0)} \ge \, 1  \, \qquad \forall \,{j}
\end{aligned}
\eeq

These transformations are combined into one step in the book Elements of Statistical Learning. \\

Since $M$ has dropped out of our optimization program, we need find it by manall checking the distance of each datapoint from the separating hyperplane.


\section{Alternative elegant formulation}

We add a constraint $t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)=1$ for the closest point - we are NOT assuming the knowledge of the point closest to the plane. We are just saying that there is a closest point and we can choose $\pmb{w},b$ such that $\pmb{w}^T\pmb{x}^{(j)}+b=1$ for that closest point. We can pick $\pmb{w},b$ such that $t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)=1$ because $\pmb{w}^T\pmb{x}+b=0$ is the same hyperplane, upto a scaling of $\pmb{w},b$. So the minimum distance to the closest point becomes $\frac{t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)}{\|\pmb{w}\|}=\frac{1}{\|\pmb{w}\|}$. We can directly optimize that (since we want to maximize the distance to the closest point), which is what the IISC notes do. Since the distance for the closest point is $\frac{1}{\|\pmb{w}\|}$, the distance of all other points should be greater than or equal to that.\\

That is we must have

\beq
\frac{t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)}{\|\pmb{w}\|} \ge \frac{1}{\|\pmb{w}\|}
\eeq

Cancel $\|\pmb{w}\|$ to get


\beq
{t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)} \ge 1
\eeq

These are our constraints.

Distances of other points is still

\beq
\frac{t^{(j)}(\pmb{w}^T\pmb{x^{(j)}}+b)}{\|\pmb{w}\|} 
\eeq
CURRENT thinking: It seems that the constraint $\pmb{w}^T\pmb{x}^{(j)}+b=1$ for the closest point or pair of points does not need to be enforced explicitly at all. All that is needed are the constraints $t_j(\pmb{w}^T\pmb{x}^{(j)}+b)=1$  for all points where $t_j$ is either +1 or -1 to make the distance positive. See svm\_multiple\_points.py. \\

PREVIOUS thinking: See section (\ref{sect:symmetric}). I think the constraint $\pmb{w}^T\pmb{x}^{(j)}+b=1$ needs to be enforced for a PAIR of closest points from the positive and negative class. (somehow, and intelligently). If it is enforced for only one point, it ends up maximizing the distance from that point only. (not sure about this) (this thinking is wrong)  in svm\_two\_point.py We can see that setting $\pmb{w}^T\pmb{x}^{(j)}+b=1$ for only one point suffices to find the separating hyperplane.

\subsection{Turning a complex optimization problem into a quadratic optimization problem}


\subsection{Soft-margin classfier: intuition}

\subsection{Sample formulation}
Let there be two points +ve class $(1,1)$ and negative class $(0,0)$. Obviously the optimal separating hyperplane is $x+y=1$. Let us pick the (0,0) point as the closest point. And let us look for solutions of the form $ax+ay-1=0$ which ensure that the $|(a,a)^T(0,0)-1|=1$. So our problem is

\ber
\max_{a} &  \qquad \frac{1}{\|(a,a)\|}\\
\text{s.t.} & (a,a)^T(1,1)-1 \,\ge\, 1
\eer

The constraint reduces to $a\,\ge\,1$ and prevents maximization by letting $a\rightarrow{0}$, and makes the hyperplane lie in between both the points.
 

\section{\label{sect:symmetric}Hyperplane is symmetric}
Let $\pmb{x}^{(1)},\pmb{x}^{(2)}$ be two points, one in the +ve class and one in the -ve class. The objective of the hyperplane is to maximine the minimum distance between the hyperplane and points $\pmb{x}^{(1)},\pmb{x}^{(2)}$. If the hyperplane is not symmetric between $\pmb{x}^{(1)},\pmb{x}^{(2)}$ then one distance is smaller than the other and that smaller distance will be our separation. By moving the hyperplane in the direction of the point with the larger distance, we can increase the minimum distance to the hyperplane. This process can continue until the distances are exactly equal.

\section{Soft margin classifier}
-Instead of explicitly enforcing constraints, penalize them.
-Useful when linear separation is impossible.

\section{SVM vs logistic regression with poly features}
- Logistic regression is hard to train with polyfeatures\\
- Kernel trick in SVM means you don't need to explicitly compute features\\
- Kernel trick comes from the dual of the quadratic optimization problem.\\
- Before deeplearning designing kernels used to be a research effort\\
- LR is probability based, SVM is not\\
- SVM does not support calculating probs directly\\

\section{Gaussian Radial basis features}
- Create new feature for every data point
- In theory it introduces infinite features (one for every data point). this is in contrast to creating polynomial features, where the number of features is not dependent on the number of data points.

\section{RKHS theorem}
- any non-separable data can be separated in an appropriate higher dimensional space.

\section{Links}
\url{https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe}

\section{Support vector regression}
``Support Vector Regression is a supervised learning algorithm that is used to predict discrete values. Support Vector Regression uses the same principle as the SVMs. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points (within a tolerance - nhg).'' \url{https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0} But what is the dependent variable? Suppose we are given data points $(x_i,y_i)$. We use a support vector regression to determine the line which contains most of the points. This line is $y=wx+b$. When we see a new point $x_{new}$, put it in and get $y_{new}$. i.e. $y_{new} = wx_{new}+b$

\section{Complexity of the primal and dual problem}
Geron, page 161, ``The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features.'' i.e,$m<n$. In general $m>n$ - so why do the dual form? it enables the kernel trick.
Chapelle ``Training a Support Vector Machine in the Primal`` page 2: Primal $\mathcal{O}(mn^2 + n^3)=\mathcal{O}(n^2(m+n))$ and Dual $\mathcal{O}(nm^2+m^3)=\mathcal{O}(m^2(n+m))$. I.e. $m<n$ dual is preferred.  


\section{Prediction}

Dual problem:

\beq
\hat{L}(\alpha) = \sum_{j=1}^{j=m}\alpha_j - \frac{1}{2}\sum_{j=1}^{j=m}\sum_{k=1}^{j=m}\alpha_j\alpha_kt_jt_k(\pmb{x}^{(j)})^T\pmb{x}^{(k)}
\eeq

This involves only the inner product of $(\pmb{x}^{(j)})^T\pmb{x}^{(k)}$. We can preplace it by any other inner product $K(\pmb{x}^{(j)},\pmb{x}^{(k)})$. This is the kernel trick. This gives

\beq
\hat{L}(\alpha) = \sum_{j=1}^{j=m}\alpha_j - \frac{1}{2}\sum_{j=1}^{j=m}\sum_{k=1}^{j=m}\alpha_j\alpha_kt_jt_kK(\pmb{x}^{(j)},\pmb{x}^{(k)})
\eeq

After $\alpha_j$ has been computed by solving the dual problem $\pmb{w},b$ appearing in the hyperplane $\pmb{w}^{T}\pmb{x} + b$ can be computed from the stationarity conditions.
(Geron 5-7)

\ber
\pdd{L}{\pmb{w}} &=& 0 \implies \pmb{w} = \sum_{j=1}^{j=m} \alpha_j t_j \pmb{x}^{(j)} \\
\pdd{L}{b}  &=& 0 \implies b = \frac{1}{n_s}\sum_{j=1}^{j=m} (1-t^{(i)}(\pmb{w}^T\cdot\pmb{x}^{(i)}))
\eer
where $n_s$ is the number of support vectors. (Geron, appendix)

Knowing $w_j$ and $b$ we can now make predictions for a new data point $\pmb{x}^{k}$ as 

\ber
\pmb{w}^T{\pmb{x}^{(k)}} + b &=& \sum_{p=1}^{n}(\sum_{j=1}^{j=m} \alpha_j t_j x_p^{(j)})x^{(k)}_{p} + b \\
&=& \sum_{j=1}^{j=m}  \alpha_j t_j ( \sum_{p=1}^{n}x_p^{(j)}x^{(k)}_{p}) + b
\eer

Replace the inner product $\sum_{p=1}^{n}x_p^{(j)}x^{(k)}_{p}$ with the kernel to get non-linear predictions

\ber
\pmb{w}^T{\pmb{x}^{(k)}} + b = \sum_{j=1}^{j=m}  \alpha_j t_j K(\pmb{x}^{(j)},\pmb{x}^{(k)}) + b
\eer

\textbf{$\alpha_j$ is non-zero only for support vectors. Geron p.g. 163}

\section{ Regularization }

In linear regression
\ber
\alpha\,&\rightarrow&\,{0} \text{, overfit }\\
\alpha\,&\rightarrow&\,{\infty} \text{,underfit}
\eer

In soft-margin SVM in Geron's (fig 5-4) and IISc formulation

\ber
C\,&\rightarrow&\,{0} \text{, underfit because margins can be as large as required} \\
C\,&\rightarrow&\,{\infty} \text{, overfit because margins are zero} 
\eer

\section{Other notes}
\url{https://jeremykun.com/2017/06/05/formulating-the-support-vector-machine-optimization-problem/}

\end{document}
