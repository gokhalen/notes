\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{url}
\begin{document}
\title{Linear regression and hypothesis testing}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Introduction}
The main reference for this is Chap 1 and Chap 2 of \url{http://home.iitk.ac.in/~shalab/course5.htm}. Consider the following relationship between the observed variable $y$ and the ``explanatory variable'' $x_1$:
\beq
y = \beta_0 + \beta_1x_1 + \epsilon
\eeq
Following what Shalab says in Chapter 1: There exist the true values of $\beta_0$ and $\beta_1$ in nature but are unknown to the experimenter. We need to estimate these true parameters.
\subsection{Least squares estimation}
\beq
y_i = \beta_0 + \beta_1x_1 + \epsilon_i
\eeq
Minimize the following any way we want (gradient descent/normal equations) to get $\beta_0,\beta_1$:
\beq
\pi = \sum_{i=1}^{i=n}\epsilon_i^2 = \sum_{i=1}^{i=n}(y_i - \beta_0 - \beta_1x_1)^2
\eeq
The parameters which are obtained by solving the above equation are called $b_0,b_1$ and our model becomes
\beq
y = b_0 + b_1x_1
\eeq
\subsection{Hypothesis testing}
Notes from Chapter 2 (pg 15) of Shalab. Sometimes we need to see if there is a significant relationship between the explanatory variables and the observed variables. The way to do this is called 'hypothesis testing'. Let us assume that we know the variance of $\epsilon_i$ i.e.
\beq
\text{var}(\epsilon_i) = \sigma^2
\eeq
Also, assume that
\beq
\epsilon_i \sim N(0,\sigma^2)
\eeq
We develop a test for the null hypothesis related to the slope parameter
\beq
H_0:\beta_1 = \beta_{10}
\eeq
Where, $\beta_{10}$ is some constant. Assuming $\sigma^2$ to be known, we know:
\ber
E(b_1)&=&\beta_1 \\
\text{var}(b_1) &=& \frac{\sigma^2}{s_{xx}}\\
s_{xx} &=& \sum_{i=1}^{n}(x_i - \bar{x})^2,\,\bar{x} = \frac{1}{n}\sum_{i=1}^{i=n}x_i
\eer
Since $b_1$ is a linear combination of normally distributed $y_i's$. So,
\beq
b_1 \sim N(\beta_1,\frac{\sigma^2}{s_{xx}})  
\eeq
Even though we do not know $\beta_1$, it apparently suffices to know that $b_1$ is normally distributed. Based on this, we can construct the following statistic
\beq
Z_1 = \frac{b_1-\beta_{10}}{\sqrt{\frac{\sigma^2}{s_{xx}}}}
\eeq
IMPORTANT:\underline{$Z_1$ is normally distributed as $N(0,1)$ when $H_0$ is true}. It seems that we need to construct a statistic (such as $Z_1$) that follows the pdf (according to which we judge whether things are true or not) when $H_0$ is true.\\
I am pretty sure that the statistic $Z_1$ is originally 
\beq
Z_1 = \frac{\beta_1-\beta_{10}}{\sqrt{\frac{\sigma^2}{s_{xx}}}}
\eeq
But we are probably allowed to replace $\beta_1$ by $b_1$, since $b_1 \sim N(\beta_1,\frac{\sigma^2}{s_{xx}})$ hence $b_1$ is centered around $\beta_1$. I'm also almost sure that $\text{var}(\beta_1)=\text{var}(b_1)$ but I don't have time to prove it right now.\\
A decision rule to test $H_1:\beta_1 \neq \beta_{10}$ can be framed as folows:
Reject $H_0$ if $|Z_1| > Z_{\alpha/2}$, where $Z_{\alpha/2}$ is the $\alpha/2$ percent points on the normal distribution. I think $\alpha/2 \in(0,0.5)$. $Z_{\alpha/2}$ is a point on the x-axis beyond which the area under the normal distribution pdf ( i.e. probability) is $\alpha/2$. i.e.
\beq
\int_{Z_{\alpha/2}}^{\infty}N(0,1)dx = \frac{\alpha}{2} \text{ and } \int_{-\infty}^{-Z_{\alpha/2}}N(0,1)dx = \frac{\alpha}{2}
\eeq
$\alpha$ is called the significance level. Typically, $\alpha = 0.05$. This means if the we're rejecting $H_0:\beta_1 = \beta_{10}$ if the probability of the statistic $Z_1$ is less than $\alpha$. In particular, if $\beta_{10}=0$, our null hypothesis becomes $H_0:\beta_1 = 0$. This means that we're postulating that no relationship exists between $x_1$ and the $y$ (because the coefficient of $x_1$ is zero). Note: our null-hypothesis is on the true parameter $\beta_1$ and not the estimated parameter $b_1$. If we reject  $H_0:\beta_1 = 0$, this means that there is a relationship between $y$ and $x_1$.\\
Shalab also talks about hypothesis testing for the intercept term $\beta_0$. It is mostly similar to this.
%
%
%
\section{Linear regression}
From IISC notes. The linear regression problem is
\beq
(X^TX)\theta^{*} = (X^Ty) 
\eeq
If features are normalized (meaned) then then we get
\ber
\frac{1}{m}(X^TX)\theta^{*} &=& \frac{1}{m}(X^Ty)  \\
C_{XX}\theta^{*} &=& C_{XY} \text{    where $C_{XX},C_{XY}$ are correlation matrices}
\eer
Features for linear regression should be (or good to be)
\begin{enumerate}
\item{Uncorrelated amongst themselves (Ensures $C_{XX}$ has full rank)}
\item{Correlated with target (ensures predictive power)}  
\end{enumerate}
Prof says $SVD$ can take care of rank-deficient matrices
%
%
%
\section{Gradient descent vs normal equations}
1) Gradient descent is cheaper - see \url{https://www.quora.com/When-would-you-use-gradient-Descent-vs-Normal-Equation}
2) Normal equations are only for linear equations. For normal equations the corresponding is the Newton's method which requires Hesssian, which is expensive to compute and use.
%
%
%
\section{Distribution of variables in regression}
From Deepak Subramani: Normal and (perhaps other distribution) of regression features and targets enables us to do uncertainity quantification on the predicted variables i.e. confidence bounds and hypothesis testing on the fitted parameters. Probably normal distribution of target and features implies normal distribution of parameters.

\section{$R2$ for non-linear models}
$R2$ doesn not make sense for non-linear models. i.e. it cannot be used to prefer one model over another.
\url{https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/} $R2$ cannot distinguish between good and bad non-linear models.

\section{Linear reg and rhs transformation}

\section{RHS transformation I}
Consider the following linear system where $\pmb{A}$ is a square invertible matrix.
\beq
\pmb{A}x = b
\eeq
Consider $f(y) = cy + d$ where $c$ is a real number and  $y,d$ are vectors. Under what conditions is
\beq
\pmb{A}f(x) \stackrel{?}{=} f(b)
\eeq
Expanding the above equation we get
\ber
\pmb{A}(cx+d) = cb+d\\
c\pmb{A}x + \pmb{A}d = cb + d\\
cb + \pmb{A}d= cb + d\\
\pmb{A}d = d
\eer
So its either $d=0$ or $d$ is an eigenvector of $A$ with eigenvalue $1$. I wasn't expecting the latter.

\textbf{Paul's comments}
Consider the case where $\pmb{A}$ has an eigenvector $d$ with eigenvalue 1 and $A$ is symmetric
\ber
x^T\pmb{A}^T &=& b^T \\
x^T\pmb{A}^Td &=& b^Td  \\
x^T\pmb{A}d &=& b^Td  \text{ ...symmetry of } \pmb{A}\\
x^Td &=& b^Td
\eer




\subsection{RHS transformation II }
Consider the linear system,
\beq
\pmb{A}x = b \text{ where } \pmb{A}\in\mathbb{R}^{m\times{n}},\,x\in\mathbb{R}^n,\,b\in\mathbb{R}^m
\eeq
which can be solved using the pseudo-inverse $(\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^T$. We have, therefore,
\beq
x = (\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^Tb
\eeq
We transform the rhs using $f(z)=cz+d$ where $c\in\mathbb{R}$ is a real number and $d\in\mathbb{R}^{m}$. We keep the matrix $\pmb{A}$ the same. We get the linear system
\beq
\pmb{A}y = cb + d 
\eeq
Which can be solved again using the pseudo-inverse
\beq
\label{eqn:ydef}
y = (\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^T(cb + d)
\eeq
If we inverse transform $y$ do we get back $x$? First we need $f^{-1}$.
\ber
f(z)=cz+d \qquad \text{let } w = cz+d \implies z =\frac{(w-d)}{c}\\
z = f^{-1}(cz+d) \implies  \frac{(w-d)}{c} = f^{-1}(w)
\eer
Hence,
\beq
f^{-1}(z) = \frac{z-d}{c}
\eeq
We get applying the above inverse to equation (\ref{eqn:ydef})
\beq
f^{-1}(y) = f^{-1}((\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^T(cb + d)) = \frac{(\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^T(cb + d) -d}{c}
\eeq
Clearly $f^{-1}(y)$ is equal to $x=(\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^Tb$ only if $(\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^Td-d = 0$. This can be accomplished if $d=0$ or if $d$ is an eigenvector of $(\pmb{A}^{T}\pmb{A})^{-1}\pmb{A}^T$ with eigenvalue 1. Leave the eigenvalue = 1 case  aside. We can say that if $f(z)=cz$ then we can transform the rhs, solve the equation, and then inverse transform back to get the solution.


\textbf{In light of the discussion above, it is not a good idea to scale the right hand side using a standard scaler in which the mean is subtracted and the result diived by the standard deviation. This transformation will, in general, not satisfy the conditions derived above.}

\subsection{Regression with a dependent variable}
Suppose we are given a feature matrix $\pmb{X}$ and observations $b_1,b_2$ which are independent and we make a third dependent observation $b_3=b_1+b_2$. Consider that we have solved the regression problems $\pmb{X}\theta_1=b_1$ and $\pmb{X}\theta_2=b_2$ for $\theta_1,\theta_2$ as
\beq
\theta_1 = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^Tb_1 \qquad \theta_2 = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^Tb_2
\eeq
Then,
\beq
\theta_3 = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^Tb_3 =(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T(b_1+b_2)  = \theta_1 + \theta_2
\eeq
So, if we are given new datapoints $\pmb{X}^{new}$ and we want to make a prediction for $b_3$, we can either sum up the predictions using $\theta_1,\theta_2$ or make an independent prediction using $\theta_3$. These will be equal. This is shown below
\ber
p_1 = \pmb{X}^{new}\theta_1 \qquad p_2 = \pmb{X}^{new}\theta_2\\
p_3 = \pmb{X}^{new}\theta_3 = \pmb{X}^{new}(\theta_1+\theta_2) = p_1 + p_2
\eer

\textbf{What if we scale the right-hand sides, compute parameters, make predictions, invert them and then add them up?}.  We have the following problems

\beq
\pmb{X}\theta_1^{'}=\frac{b_1-\bar{b}_1}{\sigma_{b_1}} \qquad \pmb{X}\theta_2^{'}=\frac{b_2-\bar{b}_2}{\sigma_{b_2}} \qquad \pmb{X}\theta_3^{'}=\frac{b_3-\bar{b}_3}{\sigma_{b_3}}
\eeq
Where $\bar{b}_i$ is the mean of $b_i$ and $\sigma_{b_i}$ is some constant. Because of what we saw in the previous section, the predictions made by scaling the rhs and then unscaling them, will not match the predictions made directly unless the transformation obeys the conditions derived in the previous section.\\
We have
\ber
\theta_1 = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_1-\bar{b}_1}{\sigma_{b_1}})\\
\theta_2 = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_2-\bar{b}_2}{\sigma_{b_2}})\\
\theta_3 = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_3-\bar{b}_3}{\sigma_{b_3}})
\eer
Now we make predictions
\ber
p_1 = \pmb{X}^{new}\theta_1 = \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_1-\bar{b}_1}{\sigma_{b_1}})\\
p_2 = \pmb{X}^{new}\theta_2 = \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_2-\bar{b}_2}{\sigma_{b_2}})\\
p_3 = \pmb{X}^{new}\theta_3 = \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_3-\bar{b}_3}{\sigma_{b_3}})
\eer
Now, we inverse transform them
\ber
\text{inv}(p_1) &=& \sigma_{b_1}\pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_1-\bar{b}_1}{\sigma_{b_1}}) + \bar{b}_1\\
&=& \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (b_1-\bar{b}_1) + \bar{b}_1 \\
%
\text{inv}(p_2) &=& \sigma_{b_2}\pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_2-\bar{b}_2}{\sigma_{b_2}}) + \bar{b}_2\\
&=& \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (b_2-\bar{b}_2) + \bar{b}_2\\
\text{inv}(p_3) &=& \sigma_{b_3}\pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (\frac{b_3-\bar{b}_3}{\sigma_{b_3}}) + \bar{b}_3\\
&=& \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (b_3-\bar{b}_3) + \bar{b}_3 
\eer
We get
\ber
\text{inv}(p_1) + \text{inv}(p_2) = & & \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (b_1-\bar{b}_1) + \bar{b}_1 \\ &+&  \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T (b_2-\bar{b}_2) + \bar{b}_2 \\
\eer
So, doing regression on $b_3$ with scaled rhs is the same as doing regression on $b_1,b_2$ with scaled rhs.
\ber
\text{inv}(p_1) + \text{inv}(p_2) &=& \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T(b_1+b_2-(\bar{b}_1 + \bar{b}_2)) + \bar{b}_2 + \bar{b}_3 \\
&=& \pmb{X}^{new}(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T(b_3 - \bar{b}_3) + \bar{b}_3 \\
&=& \text{inv}(p_3)
\eer
The result follows from
\beq
b_3 = b_1 + b_2
\eeq
and
\beq
\bar{b}_3 = \frac{1}{m}\sum_{i=1}^{m} {{}{b}_3}_i = \frac{1}{m}\sum_{i=1}^{m} ({{}{b}_1}_i+{{}{b}_2}_i) = \bar{b}_1 + \bar{b}_2
\eeq
We have abused some notation in denoting a scalar value $\bar{b}_3$ and a vector filled with $\bar{b}_3$ by the same symbol.
%
%
%
\section{Normal equations always have a solution}
\url{https://sites.math.washington.edu/~burke/crs/308/LeastSquares.pdf} (Normal\_Equations\_LeastSquares\_Washington.pdf)
\beq
\pmb{X}^T\pmb{X}\pmb{\theta} = \pmb{X}^T\pmb{y}
\eeq
$\pmb{X}^T\pmb{y} \in \text{Ran}({\pmb{X}^T})$  and $\text{Ran}(\pmb{X}^T\pmb{X})=\text{Ran}(\pmb{X}^T)$. So the system always has an equation. \\

\textbf{Property 1} Null($\pmb{X}$)$^\complement$ = Ran$(\pmb{X}^T)$ where $^\complement$ is the orthogonal complement\\

Let $\pmb{\theta}\in\text{Null}(\pmb{X})$. Hence,
\beq
\pmb{X}\pmb{\theta} = 0
\eeq
So, rows of $\pmb{X}$ are orthogonal to $\pmb{\theta}$ and hence lie in $\text{Null}(\pmb{X})^\complement$. $\text{Ran}(\pmb{X}^T)$ is the column space of $\pmb{X}^T$ which is the row space of $\pmb{X}$ which is the orthogonal complement of $\text{Null}(\pmb{X})$. This proves \textbf{Property 1}.\\

\textbf{Property: 2 } Null($\pmb{X}^T\pmb{X}$) = Null($\pmb{X}$)\\

Let $\pmb{\theta}\in\text{Null}(\pmb{X})$. Hence $\pmb{X}\pmb{\theta}=0$ and hence $\pmb{X}^T\pmb{X}\pmb{\theta}=0$ i.e. $\pmb{\theta}\in\text{Null}(\pmb{X}^T\pmb{X})$ i.e. Null($\pmb{X}$) $\in$  Null($\pmb{X}^T\pmb{X}$) \\\\
%
Let $\pmb{\theta}\in\text{Null}(\pmb{X}^T\pmb{X})$ i.e. $\pmb{X}^T\pmb{X}\pmb{\theta}=0$ i.e. $\pmb{\theta}^T\pmb{X}^T\pmb{X}\pmb{\theta}=0$ i.e. $(\pmb{X}\pmb{\theta})^T(\pmb{X}\pmb{\theta})=0$ i.e. $\pmb{\theta}\in\text{Null}(\pmb{X})$ i.e. Null($\pmb{X}^T\pmb{X}$) $\in$ Null($\pmb{X}$)\\\\
%
%
Hence Null($\pmb{X}^T\pmb{X}$) = Null($\pmb{X}$)\\\\
%
%
%
To complete the proof

\ber
\text{Ran}(\pmb{X}^T\pmb{X}) &=& \text{Ran}((\pmb{X}^T\pmb{X})^T) \qquad \text{by symmetry of } \pmb{X}^T\pmb{X}\\
&=& \text{Null}(\pmb{X}^T\pmb{X})^\complement \\
&=& \text{Null}(\pmb{X})^\complement\\
&=& \text{Ran}(\pmb{X}^T)
\eer


%
%
%
\section{Multivariate linear regression: confidence limits and hypothesis testing}
\url{https://daviddalpiaz.github.io/appliedstats/multiple-linear-regression.html}
%
%
%
\end{document}
