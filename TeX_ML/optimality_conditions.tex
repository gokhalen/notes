\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\begin{document}
\title{Optimality conditions}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Introduction}
A (slightly hand waving) proof of first and second order optimality conditions is given. Let $x_0$ be a local minimum of $f(x)$ i.e. $f(x_0) \leq f(x_0+h) \,\forall\, h $ in a small neighborhood of $x_0$. The $\leq$ is important. Changing it to $<$ does not work.
\section{Proof: first condition}
\ber
f(x_0) &\leq& f(x_0+h)  \,\forall\, h \text{ in a small neighborhood around } x \\
f(x_0) &\leq& f(x_0) + f'(x_0)h + ...\\ 
f(x_0) &\leq& f(x_0) - f'(x_0)h + ... \text{ considering a step in the opposite direction }
\eer
Neglecting higher order terms and simplifying equations (2) and (3) we get
\beq
0 \leq f'(x_0)h \qquad \text{and} \qquad 0 \leq -f'(x_0)h
\eeq
The above equations can be satisfied only for $f'(x_0)h=0$. Since $h\neq{0}$, we must have $f'(x_0)=0$. This is the first order optimality condition.
\subsection{Dropping the $\leq$}
If we assume $f(x_0)<f(x_0+h)$, then for this to occur we need $f'(x)<0$ on the left of $x_0$ and $f'(x)>0$ on the right of $x_0$. If we assume the $f'(x)$ is a continuous function, then $f'(x_0)=0$. As $f'(x)$ changes sign from left of $x_0$ to right of $x_0$ it needs to take all possible values between negative an positive. The only value between negative and positive is $0$. So, $f'(x_0)=0$.\
This is an application of the \textbf{intermediate value theorem}\footnote{https://en.wikipedia.org/wiki/Intermediate\_value\_theorem}. \\
\textit{In mathematical analysis, the intermediate value theorem states that if f is a continuous function whose domain contains the interval [a, b], then it takes on any given value between f(a) and f(b) at some point within the interval}

\section{Proof: second condition}
Using $f'(x_0)=0$, the Taylor expansion becomes
\beq
f(x_0+h) = f(x_0) + \frac{1}{2}f''(x_0)h^2 + ...
\eeq
Neglecting higher order terms, we note that since $f(x_0+h) \geq f(x_0)$ we must have $\frac{1}{2}f''(x_0)h^2 \geq 0$. This implies $f''(x_0) \geq 0$. If we assume $f(x_0+h) > f(x_0)$, we will get $f''(x_0) > 0$.
\section{Gradient is the direction of most increase}
\ber
f(\pmb{x} + h\pmb{n}) \approx f(\pmb{x}) + h\nabla{f(\pmb{x})}\cdot\pmb{n} \text{ where } \pmb{n} \text{ is a unit vector} \\
\nabla{f(\pmb{x})}\cdot\pmb{n} = |f(\pmb{x})|\cos(\theta) \text{ where } \theta \text{ is the angle between} \nabla{}f(\pmb{x}) \text{ and } \pmb{n} 
\eer
For a given positive and small $h$, $h\nabla{f(\pmb{x})}\cdot\pmb{n} = h|f(\pmb{x})|\cos(\theta) $ is maximized when $\theta=0$ ( $\pmb{n}$ is in the same direction as $\nabla{f}$) and minimized when $\theta=180^{\circ}$ (when $\pmb{n}$ is in a direction opposite to $\nabla{}f$).
%
%
%
\section{Line search}
Now that we know the direction of steepest descent, we need to know how much we need to go along that direction. Let $\pmb{g}=\nabla{}f(\pmb{x})$ and let $\pmb{H}$ be the Hessian of $f$
\ber
f(\pmb{x} + \alpha\pmb{g}) \approx f(\pmb{x}) + \alpha\pmb{g}\cdot\pmb{g} + \frac{1}{2}\alpha^2\pmb{g}\pmb{H}\pmb{g}
\eer
Becase we want to minimize $f(\pmb{x} + \alpha\pmb{g})$ we differentiate wrt $\alpha$ and set to zero to get
\ber
\dd{f}{\alpha} = 0 = \pmb{g}\cdot\pmb{g} + \alpha\pmb{g}\pmb{H}\pmb{g} \implies \alpha = -\frac{\pmb{g}\cdot\pmb{g}}{\pmb{g}\pmb{H}\pmb{g}} 
\eer
when
\ber
\text{ when }\frac{\pmb{g}\cdot\pmb{g}}{\pmb{g}\pmb{H}\pmb{g}} > 0
\eer
If $\frac{\pmb{g}\cdot\pmb{g}}{\pmb{g}\pmb{H}\pmb{g}}<0$ we are going in the direction of ascent.
Similar equation in \url{http://www.ee.iitm.ac.in/~uday/2021b-EE5121/ls.html} equation (8). \\
In case quadratic approximation is not good enough, we need to use backtracking/other approaches to improve this estimate of the step.
%
%
\section{Steepest descent and Newton}
From previous section the steepest descent update is:
\ber
\pmb{x}_{new} = \pmb{x}_{old} + \alpha\pmb{g} \text{ where } \alpha < 0 \text{ and }  \alpha = \frac{\pmb{g}\cdot\pmb{g}}{\pmb{g}\pmb{H}\pmb{g}}
\eer
Even though, Hessian information is used, this update is different from the Newton update
\ber
\pmb{x}_{new} = \pmb{x}_{old} - \pmb{H}^{-1}\nabla{f(\pmb{x}_{old})}
\eer
Which is why Steepest descent takes many steps to go to the minimum of a quadratic function, while Newton takes one step.\\
In steepest descent we are making a function that is a quadratic approximation of the original function in the direction of the gradient. For Newton, it is a quadratic approximation in all directions. In steepest descent, we choose a direction defined by the gradient and find the minimum in that direction. The global minium (usually) does not lie in the direction indicated by the gradient. \\
%
%
%
\section{Stochastic gradient with restart}
\url{https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163}
``In order to find a more stable local minimum, we can increase the learning rate from time to time, encouraging the model to “jump” from one local minimum to another if it is in a steep trough. This is the “restarts” in SGDR.''
%
%
%
\section{Proper learning rate is necessary for convergence even for convex functions}
If learning rate is not set correctly, gradient descent even convex functions will diverge. Consider the convex function $f(x)=x^2$. Gradient descent for this function is
\beq
x^{i+1} = x^{i} - \eta(2x^{i}) = (1-2\eta)x^{i}
\eeq
A reasonable condition for convergence is
\beq
\Big|\frac{x^{i+1}}{x^{i}}\Big| = |1-2\eta| < 1 \implies 0 < \eta < 1
\eeq
Because, we want the $(i+1)^{th}$ iterate to be closer to zero than the $i^{th}$ iterate. The following python code implements gradient descent for $f(x)=x^2$ starting from $x=1$
\begin{verbatim}
xx  = [1.0]
eta = 0.9
for ii in range(10):
    xx.append(xx[-1]*(1-2*eta))
print(xx)
\end{verbatim}
%
%
\section{Conjugate gradients and other algorithms}
See Goodfellow's book for a good discussion. ``For a quadratic surface, the conjugate directions ensure that the gradient along
the previous direction does not increase in magnitude. We therefore stay at the
minimum along the previous directions.''
\end{document}
