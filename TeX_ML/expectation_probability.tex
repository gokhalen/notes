\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bPi}{\mathbf{\Pi}}
\newcommand{\bQ}{{\mathbf{Q}}}
\newcommand{\bq}{{\mathbf{q}}}
\newcommand{\bK}{{\mathbf{K}}}
\newcommand{\bU}{{\mathbf{U}}}
\newcommand{\bS}{{\mathbf{S}}}
\newcommand{\bV}{{\mathbf{V}}}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{tikz}
\usepackage{cancel}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
\begin{document}
\title{Expectation}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{Expectation and other statistics. Mostly from Goodfellow}
Let a fair die be tosses. The set out outcomes is ${1,2,3,4,5,6}$. $E(x)$ is $\frac{1}{6}(1+2+3+4+5+6)=3.5$. What is $E(xE(x))$. One way to understand it is to consider $xE(x)$ as another random variable capable of taking values ${3.5,7,10.5,14,17.5,21}$. So, $E(xE(x))=\frac{1}{6}(3.5+7+10.5+14+17.5+21)=3.5*\frac{1}{6}(1+2+3+4+5+6)=3.5^2=E(x)^2$. Basiclly, $E(x)$ is number, not a random variable, and we can take it outside to get $E(x)E(x)$ 
\section{Total Expectation}
From \url{https://en.wikipedia.org/wiki/Law_of_total_expectation}.
\beq
E(X) = E(E(X|Y))
\eeq
Proof:
\ber
E(E(X|Y)) &=& E\Big[\sum_{x}xP(X=x|Y)\Big] \\
&=& \sum_y\Big[\sum_{x}xP(X=x|Y)\Big]P(Y=y) \\
&=& \sum_y\sum_xxP(X=x,Y=y) \\
&=& \sum_x x \sum_yP(X=x,Y=y) \text{ assuming sums can be interchanged } \\
&=& \sum_x x P(X=x) \\
&=& E(X)
\eer
Note: $P(X=y,Y=y)=P(X=x|Y=y)P(y)$ \text { because } $P(x\cup{y}) = P(x|y)p(y)$.
\section{Covariance and expectation}
\ber
\text{cov}(x,y) &=& E((x-E(x))(y-E(y))) \\
&=& E(xy - xE(y) - yE(x) + E(x)(E(y))) \\
&=& E(xy)- E(xE(y)) - E(yE(x)) + E(E(x)E(y)) \\
&=& E(xy) -2E(x)E(y) + E(x)E(y) \\
&=& E(xy) - E(x)E(y)
\eer
Note: $E(x),E(y)$ are no longer random variables and can be taken out of $E(xE(y))$ and $E(yE(x))$. See e.g. \url{See hhttp://www.stat.yale.edu/~pollard/Courses/241.fall97/Variance.pdf}.
\section{Expectation and independence}
$x,y$ are independent. Therefore, $P(x,y)=P(x)P(y)$
\ber
E(xy) = \sum_{x}\sum_y xy P(xy) =  \sum_{x}\sum_y xy P(x)P(y) = \sum_x xP(x) \sum_y yP(y) = E(x)E(y)
\eer
\section{Expectation and linearity}
Graham Kemp's comment on \url{https://math.stackexchange.com/questions/1810365/proof-of-linearity-for-expectation-given-random-variables-are-dependent}.
\ber
Y = X_1 + X_2 \\
E(Y) &=& \sum_{x_1,x_2}(x_1 + x_2)P(x_1,x_2) \\
&=& \sum_{x_1}\sum_{x_2}x_1P(x_1,x_2) + \sum_{x_1}\sum_{x_2}x_2P(x_1,x_2) \\
&=& \sum_{x_1}x_1\sum_{x_2}P(x_1,x_2)  + \sum_{x_2}x_2\sum_{x_1}P(x_1,x_2) \\
&=& \sum_{x_1}x_1P(x_1) + \sum_{x_2}x_2P(x_2) \text{...}P(x_2)=\sum_{x_1}P(x_1,x_2)\\
&=& E(X_1) + E(X_2)
\eer
\section{Expectation of a constant}
Interpret $E(c)$ as the random variable $x$ taking the value $c$ with 100\% probability. So,
\ber
E(c) = \sum_{x}xP(x) = cP(c) = c*1 = c
\eer
In the continuous case, the pdf is a dirac delta function $\delta(x-c)$, which yields zero probability for any interval which does not contain $c$. The expectation is
\beq
E(c) = \int x\, pdf(x) dx = \int x \delta(x-c) = c \\
\eeq
\text{( And not correct, sort of, hand waving, just to make things analogous to the discrete case)}
\beq
P(c) = \int_{c-\epsilon}^{c+\epsilon}\delta(x-c) dx = 1  \quad \epsilon\rightarrow{0}
\eeq

\section{Variance of addition}
\ber
Y &=& X_1 + X_2 \\
\text{var}(Y) &=& E(Y^2) - (E(Y))^2\\
&=& E(X_1^2 + 2X_1X_2 + X_2^2) - (E(X_1) + E(X_2))^2 \\
&=& E(X_1^2) + 2E(X_1X_2) + E(X_2^2) - (E(X_1)^2 + E(X_2)^2 + 2E(X_1)E(X_2)) \\
&=& E(X_1^2) - E(X_1)^2 + E(X_2^2) - E(X_2)^2 +2(E(X_1X_2)-E(X_1)E(X_2)) \\
&=& \text{var}(X_1) + \text{var}(X_2) +2\text{cov}(X_1,X_2)
\eer
Similarly,
\ber
\text{var}(af(x)+bg(x)) = a^2\text{var}(f) + b^2\text{var}(g) +2ab\text{cov}(f,g) 
\eer
\section{Probability}
1. Disjoint events are NOT independent. $P(A)P(B)\ne{}P(A\cap{B})$. $P(A)P(B)$ is in general not zero, $P(A\cap{B})$ is zero.\\
2. Independence cannot be understood from Venn diagrams. Cannot prove independence from data. We work with correlation. \\
3. If I know that one event has ocurred, does it increase my knowledge of another event? This is the test for independence. \\
4. Correlation: We want to see how the changes in $x$ are related to the changes in $y$.\\
\beq
\text{cor}(x,y) = \frac{\text{cov}(x,y)}{\sigma(x)\sigma(y)}\,\,\in [-1,1] 
\eeq
It lies in $[-1,1]$ because of some sort of Cauchy-Schwartz/Holder inequality.\\
5. Independence $implies$ zero correlation.\\
6. Correlation zero does not imply Independence. It just implies lack of linear relationship.\\
7. Mutual information for non-linear models.
\section{Bayesian}
\beq
P(H|D) = \frac{P(D|H)P(H)}{P(D)}
\eeq
D: Data i.e. Covid test result T+ or T-\\
H: Hypothesis i.e. is a person covid positive or negative C+ or C-\\
P(D): Evidence\\
P(D|H): Likelihood\\
P(H): belief (prior)\\
Cognitive scientists: Our brains operate like Bayes machines
\section{Mutual information}
\ber
I(X,Y) &=& \sum_{x,y}p_{X,Y}(x,y)\log\Big(\frac{p_{X,Y}(x,y)}{P_X(x)P_Y(y)}\Big)\\
&=& \sum_{x,y}p_{X,Y}(x,y)\log{p_{X,Y}(x,y)} - \sum_{x,y}p_{X,Y}(x,y)\log{p_X(x)} - \sum_{x,y}p_{X,Y}(x,y)\log{p_Y(y)} \\
&=& \sum_{x,y}p_{X,Y}(x,y)\log{p_{X,Y}(x,y)} - \sum_{x}\Big(\sum_{y}p_(X,Y)(x,y)\Big)\log{p_X(x)} \nonumber \\ && - \sum_{y}\Big(\sum_{x}p_(X,Y)(x,y)\Big)\log{p_y(y)} \\
&=& \sum_{x,y}p_{X,Y}(x,y)\log{p_{X,Y}(x,y)} - \sum_{x}p_{X}(x)\log{p_X(x)} - \sum_{y}p_{y}(y)\log{p_Y(y)} \\
&=& H(X) + H(Y) - H(X,Y)
\eer
where we have used
\beq
\sum_{y}p_{X,Y}(x,y) = p_{X}(x) \qquad \sum_{x}p_{X,Y}(x,y) = p_{Y}(y)
\eeq
We can prove
\beq
H(Y) = I(X,Y) + H(Y|X)
\eeq
That is the basis of the Venn-Diagram
\section{Conditional Entropy}
\ber
H(Y|X) &=& -\sum_{x,y}p(x,y)\log{p(y|x)} \\
&=&  -\sum_{x,y}p(x,y)\Big(\frac{p(x,y)}{p(x)}\Big) \\
&=&  -\sum_{x,y}p(x,y)\log{p(x,y)} + \sum_{x,y}p(x,y)\log{p(x)} \\
&=& -\sum_{x,y}p(x,y)\log{p(x,y)} + \sum_{x}\Big(\sum_{y}p(x,y)\Big)\log{p(x)} \\
&=& -\sum_{x,y}p(x,y)\log{p(x,y)} + \sum_{x}p(x)\Big)\log{p(x)} \\
&=& H(X,Y)- H(X)
\eer
\end{document}

