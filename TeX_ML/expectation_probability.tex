\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bPi}{\mathbf{\Pi}}
\newcommand{\bQ}{{\mathbf{Q}}}
\newcommand{\bq}{{\mathbf{q}}}
\newcommand{\bK}{{\mathbf{K}}}
\newcommand{\bU}{{\mathbf{U}}}
\newcommand{\bS}{{\mathbf{S}}}
\newcommand{\bV}{{\mathbf{V}}}
\newcommand{\ddeps}{\frac{d}{d\epsilon}\Big{|}_{\rightarrow{0}}}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hyphens]{url}
\usepackage{amssymb} 
\usepackage[utf8]{inputenc} 
%\usepackage[ngerman]{babel} 
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{tikz}
\usepackage{cancel}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
\usetikzlibrary {positioning}
\usepackage{hyperref}
\begin{document}
\title{Expectation}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
%
%
%
\section{Expectation and other statistics. Mostly from Goodfellow}
Let a fair die be tosses. The set out outcomes is ${1,2,3,4,5,6}$. $E(x)$ is $\frac{1}{6}(1+2+3+4+5+6)=3.5$. What is $E(xE(x))$. One way to understand it is to consider $xE(x)$ as another random variable capable of taking values ${3.5,7,10.5,14,17.5,21}$. So, $E(xE(x))=\frac{1}{6}(3.5+7+10.5+14+17.5+21)=3.5*\frac{1}{6}(1+2+3+4+5+6)=3.5^2=E(x)^2$. Basiclly, $E(x)$ is number, not a random variable, and we can take it outside to get $E(x)E(x)$
%
%
%
\section{Total Expectation}
From \url{https://en.wikipedia.org/wiki/Law_of_total_expectation}.
\beq
E(X) = E(E(X|Y))
\eeq
Proof:
\ber
E(E(X|Y)) &=& E\Big[\sum_{x}xP(X=x|Y)\Big] \\
&=& \sum_y\Big[\sum_{x}xP(X=x|Y)\Big]P(Y=y) \\
&=& \sum_y\sum_xxP(X=x,Y=y) \\
&=& \sum_x x \sum_yP(X=x,Y=y) \text{ assuming sums can be interchanged } \\
&=& \sum_x x P(X=x) \\
&=& E(X)
\eer
Note: $P(X=y,Y=y)=P(X=x|Y=y)P(y)$ \text { because } $P(x\cup{y}) = P(x|y)p(y)$.
%
%
%
\section{Covariance and expectation}
\ber
\text{cov}(x,y) &=& E((x-E(x))(y-E(y))) \\
&=& E(xy - xE(y) - yE(x) + E(x)(E(y))) \\
&=& E(xy)- E(xE(y)) - E(yE(x)) + E(E(x)E(y)) \\
&=& E(xy) -2E(x)E(y) + E(x)E(y) \\
&=& E(xy) - E(x)E(y)
\eer
Note: $E(x),E(y)$ are no longer random variables and can be taken out of $E(xE(y))$ and $E(yE(x))$. See e.g. \url{See hhttp://www.stat.yale.edu/~pollard/Courses/241.fall97/Variance.pdf}.
\section{Expectation and independence}
$x,y$ are independent. Therefore, $P(x,y)=P(x)P(y)$
\ber
E(xy) = \sum_{x}\sum_y xy P(xy) =  \sum_{x}\sum_y xy P(x)P(y) = \sum_x xP(x) \sum_y yP(y) = E(x)E(y)
\eer
\section{Expectation and linearity}
Graham Kemp's comment on \url{https://math.stackexchange.com/questions/1810365/proof-of-linearity-for-expectation-given-random-variables-are-dependent}.
\ber
Y = X_1 + X_2 \\
E(Y) &=& \sum_{x_1,x_2}(x_1 + x_2)P(x_1,x_2) \\
&=& \sum_{x_1}\sum_{x_2}x_1P(x_1,x_2) + \sum_{x_1}\sum_{x_2}x_2P(x_1,x_2) \\
&=& \sum_{x_1}x_1\sum_{x_2}P(x_1,x_2)  + \sum_{x_2}x_2\sum_{x_1}P(x_1,x_2) \\
&=& \sum_{x_1}x_1P(x_1) + \sum_{x_2}x_2P(x_2) \text{...}P(x_2)=\sum_{x_1}P(x_1,x_2)\\
&=& E(X_1) + E(X_2)
\eer
%
%
%
\section{Expectation of a constant}
Interpret $E(c)$ as the random variable $x$ taking the value $c$ with 100\% probability. So,
\ber
E(c) = \sum_{x}xP(x) = cP(c) = c*1 = c
\eer
In the continuous case, the pdf is a dirac delta function $\delta(x-c)$, which yields zero probability for any interval which does not contain $c$. The expectation is
\beq
E(c) = \int x\, pdf(x) dx = \int x \delta(x-c) = c \\
\eeq
\text{( And not correct, sort of, hand waving, just to make things analogous to the discrete case)}
\beq
P(c) = \int_{c-\epsilon}^{c+\epsilon}\delta(x-c) dx = 1  \quad \epsilon\rightarrow{0}
\eeq

\section{Variance of addition}
\ber
Y &=& X_1 + X_2 \\
\text{var}(Y) &=& E(Y^2) - (E(Y))^2\\
&=& E(X_1^2 + 2X_1X_2 + X_2^2) - (E(X_1) + E(X_2))^2 \\
&=& E(X_1^2) + 2E(X_1X_2) + E(X_2^2) - (E(X_1)^2 + E(X_2)^2 + 2E(X_1)E(X_2)) \\
&=& E(X_1^2) - E(X_1)^2 + E(X_2^2) - E(X_2)^2 +2(E(X_1X_2)-E(X_1)E(X_2)) \\
&=& \text{var}(X_1) + \text{var}(X_2) +2\text{cov}(X_1,X_2)
\eer
Similarly,
\ber
\text{var}(af(x)+bg(x)) = a^2\text{var}(f) + b^2\text{var}(g) +2ab\text{cov}(f,g) 
\eer
%
%
%
\section{Variance of product}
\url{https://stats.stackexchange.com/questions/52646/variance-of-product-of-multiple-independent-random-variables/52699}
%
%
%
\section{Probability}
1. Disjoint events are NOT independent. $P(A)P(B)\ne{}P(A\cap{B})$. $P(A)P(B)$ is in general not zero, $P(A\cap{B})$ is zero.\\
2. Independence cannot be understood from Venn diagrams. Cannot prove independence from data. We work with correlation. \\
3. If I know that one event has ocurred, does it increase my knowledge of another event? This is the test for independence. \\
4. Correlation: We want to see how the changes in $x$ are related to the changes in $y$.\\
\beq
\text{cor}(x,y) = \frac{\text{cov}(x,y)}{\sigma(x)\sigma(y)}\,\,\in [-1,1] 
\eeq
It lies in $[-1,1]$ because of some sort of Cauchy-Schwartz/Holder inequality.\\
5. Independence $implies$ zero correlation.\\
6. Correlation zero does not imply Independence. It just implies lack of linear relationship.\\
7. Mutual information for non-linear models.
%
%
%
\section{Bayesian}
\beq
P(H|D) = \frac{P(D|H)P(H)}{P(D)}
\eeq
D: Data i.e. Covid test result T+ or T-\\
H: Hypothesis i.e. is a person covid positive or negative C+ or C-\\
P(D): Evidence\\
P(D|H): Likelihood\\
P(H): belief (prior)\\
Cognitive scientists: Our brains operate like Bayes machines
%
%
%
\section{Mutual information}
\ber
I(X,Y) &=& \sum_{x,y}p_{X,Y}(x,y)\log\Big(\frac{p_{X,Y}(x,y)}{P_X(x)P_Y(y)}\Big)\\
&=& \sum_{x,y}p_{X,Y}(x,y)\log{p_{X,Y}(x,y)} - \sum_{x,y}p_{X,Y}(x,y)\log{p_X(x)} - \sum_{x,y}p_{X,Y}(x,y)\log{p_Y(y)} \\
&=& \sum_{x,y}p_{X,Y}(x,y)\log{p_{X,Y}(x,y)} - \sum_{x}\Big(\sum_{y}p_(X,Y)(x,y)\Big)\log{p_X(x)} \nonumber \\ && - \sum_{y}\Big(\sum_{x}p_(X,Y)(x,y)\Big)\log{p_y(y)} \\
&=& \sum_{x,y}p_{X,Y}(x,y)\log{p_{X,Y}(x,y)} - \sum_{x}p_{X}(x)\log{p_X(x)} - \sum_{y}p_{y}(y)\log{p_Y(y)} \\
&=& H(X) + H(Y) - H(X,Y)
\eer
where we have used
\beq
\sum_{y}p_{X,Y}(x,y) = p_{X}(x) \qquad \sum_{x}p_{X,Y}(x,y) = p_{Y}(y)
\eeq
We can prove
\beq
H(Y) = I(X,Y) + H(Y|X)
\eeq
That is the basis of the Venn-Diagram
%
%
%
\section{Conditional Entropy}
\ber
H(Y|X) &=& -\sum_{x,y}p(x,y)\log{p(y|x)} \\
&=&  -\sum_{x,y}p(x,y)\Big(\frac{p(x,y)}{p(x)}\Big) \\
&=&  -\sum_{x,y}p(x,y)\log{p(x,y)} + \sum_{x,y}p(x,y)\log{p(x)} \\
&=& -\sum_{x,y}p(x,y)\log{p(x,y)} + \sum_{x}\Big(\sum_{y}p(x,y)\Big)\log{p(x)} \\
&=& -\sum_{x,y}p(x,y)\log{p(x,y)} + \sum_{x}p(x)\Big)\log{p(x)} \\
&=& H(X,Y)- H(X)
\eer
\section{$E[(x-b)^2]$}
$b$ is a constant 
\ber
E[(x-b)^2] &=& E[(x-E(x)+E(x)-b)^2] \\
&=& E[(x-E(x))^2 + (E(x)-b)^2 + 2(x-E(x))(E(x)-b)]\\
&=& E[(x-E(x))^2] + E[(E(x)-b)^2] + 2E[(x-E(x))(E(x)-b)]
\eer
Now, $(E(x)-b)$ is a constant because $E(x)$ is a constant and $b$ is a constant.
\ber
E[(x-b)^2] &=& E[(x-E(x))^2] + (E(x)-b)^2 + 2(E(x)-b)E[(x-E(x))] \\
&=& E[(x-E(x))^2] + (E(x)-b)^2 + 2(E(x)-b)(E(x)-EE(x)) \\
&=& E[(x-E(x))^2] + (E(x)-b)^2 + 2(E(x)-b)\overbrace{(E(x)-E(x))}^{0}\\
&=& E[(x-E(x))^2] + (E(x)-b)^2
\eer
Since the second term is non-negative
\ber
E[(x-b)^2]\ge E[(x-E(x))^2]
\eer
Hence, by inspection the minimum value is obtained for $b=E(x)$.
%
%
%
\section{Expectation of a function of a random variable}
See Goodfellow section 3.12. Let $y=g(x)$
\beq
E(x) = \int xp_x(x)\,dx
\eeq
Analogously,
\beq
E(y) = \int yp_y(y)\,dy = \int g(x) p_x(x)\,dx \qquad \text{ because } |p_ydy|=|p_xdx|
\eeq
%
%
%
\section{Marginal distribution definition}
\url{https://en.wikipedia.org/wiki/Marginal_distribution}
`''Given a known joint distribution of two discrete random variables, say, X and Y, the marginal distribution of either variable – X for example — is the probability distribution of X when the values of Y are not taken into consideration.`''
\ber
p_X(x_i) = \sum_{j}p(x_i,y_j)
\eer
%
%
%
\section{MSE: Stanley Chan: Intro Prob for DS pg 425}
Claim:
\beq
\mathbb{E}(\pmb{e}^T\pmb{H}^T\pmb{e}') = 0
\eeq
Proof:
\beq
LHS=\mathbb{E}_{e,e'}(\sum_{ij}e_iH_{ij}{e'}_{j}) = \sum_{ij}\mathbb{E}_{e,e'}(e_iH_{ij}{e'}_{j})
\eeq
$H_{ij}$ is a constant and can be taken out of the expectation.
\beq
LHS = \sum_{ij}H_{ij}\mathbb{E}_{e,e'}(e_i{e'}_{j}) = \sum_{ij}H_{ij}\mathbb{E}_{e}(e_i)\mathbb{E}_{e'}({e'}_j) = 0
\eeq
Because of independence of $e$ and $e'$. And the product is zero because $\mathbb{E}_{e}(e_i)=\mathbb{E}_{e'}({e'}_j)=0$
%
%
%
\section{Probability integral transform: how to sample from arbitrary distribution}
From: \url{https://en.m.wikipedia.org/wiki/Probability_integral_transform}\\
1. $Y$ is a uniformly distributed random variable in $[0,1]$. Its pdf $p_Y$ is $1$ on $[0,1]$ and $0$ elsewhere. Its cdf $F_Y$ is
\beq
F_Y (y) = \int_{0}^y 1 dy = y
\eeq
We can also show that if $F_Y(y)=y$ its pdf is 1.\\
\beq
F_Y(y) = y \implies \int_{0}^y p_X dx  = y \implies p_x = 1
\eeq
2. Let $X$ be a random variable with cdf $F_X$\\
3. Let $Z=F_X(X)$ be another random variabale. We want to determine its cdf $F_Z$.
\ber
F_Z(z) &=& P(Z \le z)  \qquad \text{by definition of cdf, $P$ is probability} \\
&=& P(F_X(X) \le z) \\
&=& P( X \le F_X^{-1}(z))\\
&=& F_X(F_X^{-1}(z)) \qquad \text{by definition of cdf} \\
&=& z
\eer
Hence, by point 1, $z$ is uniformly distributed.\\
4. If $Z$ is uniformly distributed on $[0,1]$ as it is in a random number generator on a computed, we can do $X=F_X^{-1}(Z)$ to get a random variable $X$ distributed according to cdf $F_X$. Sometimes $F_X^{-1}$
is hard to calculate. This might make sampling from a distribution governed by $F_X$ hard. Hence, we might need to use a easier distribution to sample from. This motivates \textit{importance sampling}.
%
%
%
\section{pdf with maximum entropy}
\beq
H(x) = -\int_{a}^{b}p(x)\log{p(x)}dx \\
\eeq
Introduce Lagrangian and look for its stationary points
\beq
L(p(x),\lambda(x)) = -\int_{a}^{b}p(x)\log{p(x)}dx  + \lambda(x)\Big(\int_{a}^{b}p(x)dx - 1 \Big)\\
\eeq
Take derivatives wrt $p(x)$ and $\lambda(x)$ and set them to zero. Also drop spatial dependence for clarity
\beq
\label{eqn:stat1}
\delta{}L_{\lambda} = \ddeps L(p,\lambda+\epsilon\delta\,\lambda)=\delta\lambda\Big(\int_a^b {p\,}dx-1\Big) = 0 \, \implies \,\int_a^b {p\,}dx-1=0
\eeq
%
% 
\ber
\delta{}L_u = \ddeps L(p+\epsilon\delta{p},\lambda) = -\int_a^b\Big(\log{p} + p\frac{1}{p} -\lambda\Big)\,\delta{p}\,dx \\
= -\int_a^b\Big(\log{p} + 1 -\lambda\Big)\,\delta{p}\,dx \stackrel{set}{=} 0 \,\,\forall\,\delta{p}
\eer
This yields
%
%
\beq
\label{eqn:stat2}
p = -\exp(1-\lambda)
\eeq
%
%
Using the above in equation (\ref{eqn:stat1}) yields
\beq
-\int_a^b\exp(1-\lambda)=1 \implies \int_a^b\exp(1-\lambda)=-1
\eeq
The integrand $1-\lambda$ is positive, and $b>a$ so we will need to let $\lambda$ take complex values. We need
%
%
\beq
1-\lambda = \log\Big(\frac{-1}{b-a}\Big)
\eeq
Using the above in (\ref{eqn:stat2}) yields
\beq
p = \frac{1}{b-a}
\eeq
which is a uniform distribution. Therefore a uniform distribution maximizes entropy.
%
%
%
\section{PDF of sum of RVs}
\url{https://stats.stackexchange.com/questions/124085/why-does-convolution-work}
%
%
%
\end{document}

