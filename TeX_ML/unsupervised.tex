\documentclass{article}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\ber}{\begin{eqnarray}}
\newcommand{\eer}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}
\newcommand{\dd}[2]{\frac{d}{d{#2}}{(#1)} }
\newcommand{\ddeps}{\frac{d}{d\epsilon}\Big{|}_{\rightarrow{0}}}
\newcommand{\pdd}[2]{\frac{\partial{#1}}{\partial{#2}}}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\begin{document}
\title{Unsupervised}
\author{Nachiket Gokhale}
\date{\today}
\maketitle
\section{K-means clustering}
1. Algorithm is guaranteed to converge but may not convert to the correct solution. To tackle this, we need an ensemble.\\
2 Scaling issues. Data MUST be scaled.\\
3. Algorithm stops when no centroids move\\
4. Goodness of clustering: Inertia (lower the intertia, better is clustering). But, you can use $k=n$ and inertia will be zero. (overfitting) \textbf{Understand Silhouette coeff}\\
5. Prof's comment in chat: But note that means is guaranteed to converge only for Euclidean distances\\
6. K-means fit time is high but can be parallelized using OpenMP


\section{GMM}
Learn means and standard deviation from the data. Expensive to train. \url{https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e} Note: Itâ€™s worth noting, that the algorithm is susceptible to local maxima. (from previous url) \url{https://www.geeksforgeeks.org/gaussian-mixture-model/}. (Geeks for geeks is the explanation closest to an algorithm). My understanding is as follows (based on geeksforgeeks)

\begin{enumerate}
\item{Guess parameters for the mixture: $\mu_k$, $\Sigma_{k}$, $\pi_k$.}
\item{\textbf{Estimation Step:} Assign each data point to the cluster for which the probability is highest. This enables the computation of the latent variables $\gamma_k$.}
\item{\textbf{Maximization Step} Update the values of $\mu_k$, $\Sigma_{k}$, $\pi_k$ using maximization of likelihood}
\item{Continue steps 2 and 3 until likelihood is maximized sufficiently}  
\end{enumerate}


\section{DB-Scan}
1. DBScan figures out number of clusters for itself.
2. DBSCAN cannot predict which cluster a new instance belongs too
3. Bigger $\epsilon$ means fewer clusters. Smaller $\epsilon$ more clusters
4. More min\_samples means bigger,fewer clusters. Smaller min\_samples more clusters. \url{https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31}. Think of it this way: Suppose $\epsilon=1$ and we have two clusters in the form of circles. These clusters are widely separated and have 500 and 300 samples respectively. Now if min\_samples is 300 then 2 clusters will be formed. If min\_samples is 500 then only one cluster will be formed. 

\section{Anamoly detection}
\textbf{Anamoly} we don't want. \textbf{Novelty} we want. Means and GMMs for the model are learned from the data.

\section{Isolation forests}
- How is splitting done without knowing labels?

\end{document}
